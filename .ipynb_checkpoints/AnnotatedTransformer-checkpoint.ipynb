{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a429510",
   "metadata": {
    "id": "9a429510",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "<center><h1>万字逐行解析与实现Transformer，并进行德译英实战</h1> </center>\n",
    "\n",
    "# 本文内容\n",
    "\n",
    "本文代码来源于[该链接](https://github.com/harvardnlp/annotated-transformer/)，该作者对Transformer进行了详细解释，并进行了德译英实战，且增加了许多注解。但对于像我这样的新手来讲还是有些难度，所以我在该作者代码与注释的基础上，进一步增加注释，将英文注释改为中文注释，且去掉了一些与理解Transformer无关的代码（例如并行计算等），方便大家理解Transformer。\n",
    "\n",
    "该教程需要读者对Transformer有一些了解。\n",
    "\n",
    "本篇中可能有些注释比较难懂，请不要慌张，可以先打个标记，因为大部分都会在后面对其进行讲解。\n",
    "\n",
    "如果本篇有什么错误的地方或写的不清楚的地方，欢迎在评论区或者issue模块提出来，我会进行改正或进一步解释。\n",
    "\n",
    "本文主要内容有：\n",
    "\n",
    "1. Transformer各模块的逐行代码实现\n",
    "2. Transformer逐行代码的详细注释\n",
    "3. Transformer的训练和推理\n",
    "4. 利用Transformer进行德译中实战\n",
    "\n",
    "你可以通过[Google Colab](https://drive.google.com/file/d/172BrYvcJpQc9eIciZpyjc7RpqPDWs6pe/view?usp=sharing)直接运行以下代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0e680",
   "metadata": {
    "id": "edd0e680"
   },
   "source": [
    "# 环境准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ed0c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:17.534797Z",
     "iopub.status.busy": "2022-05-02T01:25:17.533699Z",
     "iopub.status.idle": "2022-05-02T01:25:17.538114Z",
     "shell.execute_reply": "2022-05-02T01:25:17.537128Z"
    },
    "id": "493ed0c2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "本教程使用到的类库版本如下：\n",
    "\n",
    "```\n",
    "pandas==1.3.5\n",
    "torch==1.11.0+cu113\n",
    "torchdata==0.3.0\n",
    "torchtext==0.12\n",
    "spacy==3.2\n",
    "altair==4.1\n",
    "jupytext==1.13\n",
    "flake8\n",
    "black\n",
    "GPUtil\n",
    "```\n",
    "\n",
    "一定要按照该版本进行安装，尤其是torchtext与torchdata，否则后面会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pandas==1.3.5 in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.5) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.15.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement torch==1.11.0+cu113 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0)\u001B[0m\n",
      "\u001B[31mERROR: No matching distribution found for torch==1.11.0+cu113\u001B[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torchdata==0.3.0\n",
      "  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
      "\u001B[K     |████████████████████████████████| 47 kB 3.1 MB/s \n",
      "\u001B[?25hCollecting urllib3>=1.25\n",
      "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
      "\u001B[K     |████████████████████████████████| 139 kB 12.9 MB/s \n",
      "\u001B[?25hCollecting torch==1.11.0\n",
      "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 750.6 MB 10 kB/s \n",
      "\u001B[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata==0.3.0) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchdata==0.3.0) (4.1.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata==0.3.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata==0.3.0) (2.10)\n",
      "Collecting urllib3>=1.25\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001B[K     |████████████████████████████████| 127 kB 57.6 MB/s \n",
      "\u001B[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata==0.3.0) (2022.6.15)\n",
      "Installing collected packages: urllib3, torch, torchdata\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.0+cu113\n",
      "    Uninstalling torch-1.12.0+cu113:\n",
      "      Successfully uninstalled torch-1.12.0+cu113\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.11.0 which is incompatible.\n",
      "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.11.0 which is incompatible.\n",
      "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.11.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001B[0m\n",
      "Successfully installed torch-1.11.0 torchdata-0.3.0 urllib3-1.25.11\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torchtext==0.12\n",
      "  Downloading torchtext-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "\u001B[K     |████████████████████████████████| 10.4 MB 7.4 MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.12) (1.21.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.12) (4.64.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.12) (2.23.0)\n",
      "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.12) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchtext==0.12) (4.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.12) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.12) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.12) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.12) (2.10)\n",
      "Installing collected packages: torchtext\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.13.0\n",
      "    Uninstalling torchtext-0.13.0:\n",
      "      Successfully uninstalled torchtext-0.13.0\n",
      "Successfully installed torchtext-0.12.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting spacy==3.2\n",
      "  Downloading spacy-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001B[K     |████████████████████████████████| 6.0 MB 6.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (3.3.0)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
      "\u001B[K     |████████████████████████████████| 660 kB 52.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (0.6.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (1.0.7)\n",
      "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (2.4.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (0.4.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (0.7.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (1.0.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (4.64.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (57.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (1.21.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (2.23.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (0.9.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 10.1 MB 36.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2) (21.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.2) (5.2.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2) (2022.6.15)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2) (2.0.1)\n",
      "Installing collected packages: typing-extensions, pydantic, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.1\n",
      "    Uninstalling pydantic-1.9.1:\n",
      "      Successfully uninstalled pydantic-1.9.1\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.0\n",
      "    Uninstalling thinc-8.1.0:\n",
      "      Successfully uninstalled thinc-8.1.0\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.1\n",
      "    Uninstalling spacy-3.4.1:\n",
      "      Successfully uninstalled spacy-3.4.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.11.0 which is incompatible.\n",
      "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.2.0 which is incompatible.\u001B[0m\n",
      "Successfully installed pydantic-1.8.2 spacy-3.2.0 thinc-8.0.17 typing-extensions-3.10.0.2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "typing_extensions"
        ]
       }
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting altair==4.1\n",
      "  Downloading altair-4.1.0-py3-none-any.whl (727 kB)\n",
      "\u001B[K     |████████████████████████████████| 727 kB 6.9 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.7/dist-packages (from altair==4.1) (1.3.5)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair==4.1) (4.3.3)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair==4.1) (0.12.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair==4.1) (0.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from altair==4.1) (1.21.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair==4.1) (2.11.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18->altair==4.1) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18->altair==4.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.18->altair==4.1) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair==4.1) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1) (3.10.0.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1) (5.9.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1) (0.18.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1) (4.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->altair==4.1) (3.8.1)\n",
      "Installing collected packages: altair\n",
      "  Attempting uninstall: altair\n",
      "    Found existing installation: altair 4.2.0\n",
      "    Uninstalling altair-4.2.0:\n",
      "      Successfully uninstalled altair-4.2.0\n",
      "Successfully installed altair-4.1.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting jupytext==1.13\n",
      "  Downloading jupytext-1.13.0-py3-none-any.whl (295 kB)\n",
      "\u001B[K     |████████████████████████████████| 295 kB 7.9 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from jupytext==1.13) (3.13)\n",
      "Collecting mdit-py-plugins\n",
      "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
      "\u001B[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
      "\u001B[?25hCollecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting markdown-it-py~=1.0\n",
      "  Downloading markdown_it_py-1.1.0-py3-none-any.whl (83 kB)\n",
      "\u001B[K     |████████████████████████████████| 83 kB 2.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from jupytext==1.13) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from markdown-it-py~=1.0->jupytext==1.13) (3.10.0.2)\n",
      "Requirement already satisfied: attrs<22,>=19 in /usr/local/lib/python3.7/dist-packages (from markdown-it-py~=1.0->jupytext==1.13) (21.4.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.7/dist-packages (from nbformat->jupytext==1.13) (5.1.1)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->jupytext==1.13) (4.11.1)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->jupytext==1.13) (2.16.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->jupytext==1.13) (4.3.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->jupytext==1.13) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->jupytext==1.13) (5.9.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->jupytext==1.13) (4.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->jupytext==1.13) (3.8.1)\n",
      "Installing collected packages: markdown-it-py, toml, mdit-py-plugins, jupytext\n",
      "Successfully installed jupytext-1.13.0 markdown-it-py-1.1.0 mdit-py-plugins-0.3.0 toml-0.10.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting flake8\n",
      "  Downloading flake8-5.0.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001B[K     |████████████████████████████████| 61 kB 271 kB/s \n",
      "\u001B[?25hCollecting pyflakes<2.6.0,>=2.5.0\n",
      "  Downloading pyflakes-2.5.0-py2.py3-none-any.whl (66 kB)\n",
      "\u001B[K     |████████████████████████████████| 66 kB 4.5 MB/s \n",
      "\u001B[?25hCollecting mccabe<0.8.0,>=0.7.0\n",
      "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting importlib-metadata<4.3\n",
      "  Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting pycodestyle<2.10.0,>=2.9.0\n",
      "  Downloading pycodestyle-2.9.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001B[K     |████████████████████████████████| 41 kB 434 kB/s \n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3->flake8) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3->flake8) (3.8.1)\n",
      "Installing collected packages: pyflakes, pycodestyle, mccabe, importlib-metadata, flake8\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.12.0\n",
      "    Uninstalling importlib-metadata-4.12.0:\n",
      "      Successfully uninstalled importlib-metadata-4.12.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible.\u001B[0m\n",
      "Successfully installed flake8-5.0.0 importlib-metadata-4.2.0 mccabe-0.7.0 pycodestyle-2.9.0 pyflakes-2.5.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting black\n",
      "  Downloading black-22.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.4 MB 8.1 MB/s \n",
      "\u001B[?25hCollecting platformdirs>=2\n",
      "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
      "Collecting typed-ast>=1.4.2\n",
      "  Downloading typed_ast-1.5.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
      "\u001B[K     |████████████████████████████████| 843 kB 55.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black) (3.10.0.2)\n",
      "Collecting click>=8.0.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001B[K     |████████████████████████████████| 96 kB 7.3 MB/s \n",
      "\u001B[?25hCollecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Collecting pathspec>=0.9.0\n",
      "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click>=8.0.0->black) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click>=8.0.0->black) (3.8.1)\n",
      "Installing collected packages: typed-ast, platformdirs, pathspec, mypy-extensions, click, black\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\n",
      "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.2.0 which is incompatible.\u001B[0m\n",
      "Successfully installed black-22.6.0 click-8.1.3 mypy-extensions-0.4.3 pathspec-0.9.0 platformdirs-2.5.2 typed-ast-1.5.4\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "Building wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=905fd0bd6baf847983264951598e293b1f6d781346b78d707be136c591a1e694\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.3.5\n",
    "!pip install torch==1.11.0+cu113\n",
    "!pip install torchdata==0.3.0\n",
    "!pip install torchtext==0.12\n",
    "!pip install spacy==3.2\n",
    "!pip install altair==4.1\n",
    "!pip install jupytext==1.13\n",
    "!pip install flake8\n",
    "!pip install black\n",
    "!pip install GPUtil"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NR8NyMXsVmQ2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1659239043457,
     "user_tz": -480,
     "elapsed": 145503,
     "user": {
      "displayName": "Garen Snail",
      "userId": "12776734623535172445"
     }
    },
    "outputId": "6846a651-ad69-48b5-c431-e5ef88d8e2e8"
   },
   "id": "NR8NyMXsVmQ2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "除了上面的库外，还需要用到spacy进行分词。关于spacy可参考[该文章](https://blog.csdn.net/zhaohongfei_358/article/details/125469155)。在安装完spacy后，需要安装德语和英语的库，命令如下："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "G1kR8Uf4VmQ3"
   },
   "id": "G1kR8Uf4VmQ3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting de-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 19.1 MB 7.0 MB/s \n",
      "\u001B[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.64.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.21.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (57.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.23.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.25.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.1)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.2.0\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001B[K     |████████████████████████████████| 13.9 MB 8.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUES0cbSVmQ3",
    "outputId": "0cc9c823-19b3-45d0-df0f-e7104d6682e3"
   },
   "id": "IUES0cbSVmQ3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来导入需要用到的类库："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "fYRaRSWGVmQ4"
   },
   "id": "fYRaRSWGVmQ4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3deb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:17.560273Z",
     "iopub.status.busy": "2022-05-02T01:25:17.559273Z",
     "iopub.status.idle": "2022-05-02T01:25:18.690005Z",
     "shell.execute_reply": "2022-05-02T01:25:18.690769Z"
    },
    "id": "1bf3deb7",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Pad: 用于对句子进行长度填充，官方地址为：https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "# copy: 用于对模型进行深拷贝\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "# 和matplotlib.pypolt类似，用于绘制统计图，但功能功能强大\n",
    "# 可以绘制可交互的统计图。官网地址为：https://altair-viz.github.io/getting_started/overview.html\n",
    "import altair as alt\n",
    "# 用于将 iterable 风格的 dataset 转为 map风格的 dataset，详情可参考：https://blog.csdn.net/zhaohongfei_358/article/details/122742656\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# 用于构建词典\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "# datasets：用于加载Multi30k数据集\n",
    "import torchtext.datasets as datasets\n",
    "# spacy: 一个易用的分词工具，详情可参考：https://blog.csdn.net/zhaohongfei_358/article/details/125469155\n",
    "import spacy\n",
    "# GPU工具类，本文中用于显示GPU使用情况\n",
    "import GPUtil\n",
    "# 用于忽略警告日志\n",
    "import warnings\n",
    "\n",
    "# 设置忽略警告\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60359a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.702574Z",
     "iopub.status.busy": "2022-05-02T01:25:18.701680Z",
     "iopub.status.idle": "2022-05-02T01:25:18.704131Z",
     "shell.execute_reply": "2022-05-02T01:25:18.704839Z"
    },
    "id": "60359a1a"
   },
   "outputs": [],
   "source": [
    "# 用于验证时，不进行参数更新。\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "# 用于验证时，不进行学习率调整。\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b000ba74",
   "metadata": {
    "id": "b000ba74"
   },
   "source": [
    "# 背景介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b3f4b",
   "metadata": {
    "id": "af8b3f4b"
   },
   "source": [
    "# Part 1: 模型架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe7928",
   "metadata": {
    "id": "4ffe7928"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "大部分序列模型(neural sequence transduction models)都是encoder-decoder结构。encoder负责将一个符号表示的输入序列 $(x_1, ..., x_n)$ 映射为一个连续表示的序列$\\mathbf{z} = (z_1, ...,\n",
    "z_n)$。然后将$\\mathbf{z}$作为Decoder的其中一个输入，decoder会一次一个的产生字符输出序列（output sequence of symbols）$(y_1,...,y_m)$。在每个时刻，模型都是自回归的（auto-regressive），也就是上一个时刻的产生的字符，作为写一个时刻额外的输入。\n",
    "\n",
    "而Transformer也是这样的encoder-decoder结构，一个标准的EncoderDecoder模型如下："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "KmAfxMc4VmQ6"
   },
   "id": "KmAfxMc4VmQ6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7e57e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.716980Z",
     "iopub.status.busy": "2022-05-02T01:25:18.716023Z",
     "iopub.status.idle": "2022-05-02T01:25:18.718971Z",
     "shell.execute_reply": "2022-05-02T01:25:18.718190Z"
    },
    "id": "90e7e57e"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    一个标准的EncoderDecoder模型。在本教程中，这么类就是Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        \"\"\"\n",
    "        encoder: Encoder类对象。Transformer的Encoder\n",
    "        decoder： Decoder类对象。 Transformer的Decoder\n",
    "        src_embed: Embeddings类对象。 Transformer的Embedding和Position Encoding\n",
    "                   负责对输入inputs进行Embedding和位置编码\n",
    "        tgt_embed: Embeddings类对象。 Transformer的Embedding和Position Encoding\n",
    "                   负责对“输入output”进行Embedding和位置编码\n",
    "        generator: Generator类对象，负责对Decoder的输出做最后的预测（Linear+Softmax）\n",
    "        \"\"\"\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        src: 未进行word embedding的句子，例如`[[ 0, 5, 4, 6, 1, 2, 2 ]]`\n",
    "             上例shape为(1, 7)，即batch size为1，句子词数为7。其中0为bos，\n",
    "             1为eos, 2为pad\n",
    "\n",
    "        tgt: 未进行word embedding的目标句子，例如`[[ 0, 7, 6, 8, 1, 2, 2 ]]`\n",
    "\n",
    "        src_mask: Attention层要用的mask，对于source来说，就是要盖住非句子的部分，\n",
    "                  例如`[[True,True,True,True,True,False,False]]`。相当于对上面\n",
    "                  `[[ 0, 5, 4, 6, 1, 2, 2 ]]`中最后的`2,2`进行掩盖。\n",
    "\n",
    "        tgt_mask: Decoder的Mask Attention层要用的。该shape为(N, L, L)，其中N为\n",
    "                  batch size, L为target的句子长度。例如(1, 7, 7)，对于上面的例子，\n",
    "                  值为：\n",
    "                  [True,False,False,False,False,False,False], # 从该行开始，每次多一个True\n",
    "                  [True,True,False,False,False,False,False],\n",
    "                  [True,True,True,False,False,False,False],\n",
    "                  [True,True,True,True,False,False,False],\n",
    "                  [True,True,True,True,True,False,False], # 由于该句一共5个词，所以从该行开始一直都只是5个True\n",
    "                  [True,True,True,True,True,False,False],\n",
    "                  [True,True,True,True,True,False,False],\n",
    "        \"\"\"\n",
    "\n",
    "        # 注意，这里的返回是Decoder的输出，并不是Generator的输出，因为在EncoderDecoder\n",
    "        # 的forward中并没有使用generator。generator的调用是放在模型外面的。\n",
    "        # 这么做的原因可能是因为Generator不算是Transformer的一部分，它只能算是后续处理\n",
    "        # 分开来的话也比较方便做迁移学习。另一方面，推理时只会使用输出的最后一个tensor送给\n",
    "        # generator，而训练时会将所有输出都送给generator。\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        该encode做三件事情：\n",
    "        1. 对src进行word embedding\n",
    "        2. 将word embedding与position encoding相加\n",
    "        3. 通过Transformer的多个encoder层计算结果，输出结果称为memory\n",
    "        \"\"\"\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        \"\"\"\n",
    "        memory: Transformer Encoder的输出\n",
    "\n",
    "        decoder和encoder执行过程基本一样：\n",
    "        1. 对src进行word embedding\n",
    "        2. 将word embedding与position encoding相加\n",
    "        3. 通过Transformer的多个decoder层计算结果\n",
    "\n",
    "        当完成decoder的计算后，接下来可以使用self.generator（nn.Linear+Softmax）来进行最后的预测。\n",
    "        \"\"\"\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "上述的代码中包含几个重要的模块，与Transformer的对应关系如下图：\n",
    "\n",
    "<img src=\"./images/transfomer_component.png\" width=\"400\">\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "XeGTeHyHVmQ7"
   },
   "id": "XeGTeHyHVmQ7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们根据上图来对Transformer进行拆解：\n",
    "\n",
    "1. **模型的输入**：Transformer模型的输入包含两部分：① Inputs ② Outputs。这里可能就会问了Outputs不是输出吗？怎么成输入了？且看后面解释\n",
    "    1.1 **Inputs**: Inputs可以理解为原始文本，例如我们可以将“I love you”根据字典对应的index转换为数字后（例如`[5, 4, 6]`），作为inputs传给Transformer的Encoder层。\n",
    "    1.2 **Outputs**: Outputs是上一次Decoder的输出。例如我们在做机器翻译的时候，Encoder接收的是一个完整的句子（假设为I love you），然后输出一个`memory`(在RNN里叫Hidden State)，但是Decoder并不是一下子就能输出“我爱你”，它和RNN一样，要一个字一个字输出。而Outputs就是上次的输出。（这里看不懂没关系，后面会完整的举个例子）\n",
    "2. **Embedding**: 在Inputs和Outputs的上面有两个Embedding，`Input Embedding`和`Output Embedding`，它们的作用是对字符进行编码。如果你不是特别了解，可以参考[该文章](https://blog.csdn.net/zhaohongfei_358/article/details/122809709)。通常Embedding层使用`nn.Embedding`就可以了。\n",
    "3. **Positional Encoding**: Transformer和RNN相比，一个重要的提升就是Transformer可以并行计算，其实就是Transformer的Encoder可以一次性接收整个句子，而RNN需要一个词一个词接收。但一次性接收整个句子就会淡化句子中词与词的位置关系，例如 “我爱你”和“爱你我” 可能对网络来说没什么不一样的。 为了解决这个问题，Transformer在Embedding后给每个词增加了位置成分。例如：可以让 `I` 向量的每个值都加0.1，`Love`加0.2，`you`加0.3。 当然，Transformer使用的机制并不是像这个例子那么简单，但道理是一样的。\n",
    "4. **Encoder**：Encoder就是将Inputs进行编码，在效果上和RNN的Encoder应该也没啥不一样的，但因为增加了Attention机制，Performance有了大幅提升。注意这里的 $Nx$ 指的是有N个这样的Encoder层叠加。\n",
    "5. **Decoder**: 和RNN类似，接收“Encoder的输出（称为Memory）和上一次Decoder的输出”作为输入，然后进行一系列运算得到输出。\n",
    "6. **Generator**：Decoder的输出并不能直接拿来用，还要在经过一个线性层才能得到你想要的结果。有点类似CNN中，卷积层之后需要再接一个线性层做分类。注意：在推理时，generator使用的并不是decoder的所有输出，而是最后一个`out[:, -1]`，而在训练时，则是使用Decoder的全部输出。\n",
    "7. **Output Probabilities**: 这个就是线性层后经过Softmax的概率分布。最大的值对应的index，然后再去字典中查询，就知道预测的词是什么了。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "UreJo0_cVmQ7"
   },
   "id": "UreJo0_cVmQ7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "有了上述对Transformer各层的解释，我们来看一下Transformer实际是怎么使用的。这里还使用上述翻译的例子：\n",
    "\n",
    "首先，假设我们有两个字典：\n",
    "\n",
    "- `[0(<bos>), 1(<eos>), 2(<pad>), 3(<unk>), 4(Love), 5(I), 6(You), ..., 100(other)]`\n",
    "- `[0(<bos>), 1(<eos>), 2(<pad>), 3(<unk>), 4(爱), 5(我), 6(你), ..., 100(其他)]`\n",
    "\n",
    "\n",
    "\n",
    "则我们翻译的第一步如下图：\n",
    "\n",
    "<img src=\"./images/encoder.png\" width=\"500\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "VmfyHScnVmQ7"
   },
   "id": "VmfyHScnVmQ7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "上图为Encoder的执行过程：\n",
    "\n",
    "1. 首先我们根据字典将`I Love you` 转变为了向量 `[[ 0, 5, 4, 6, 1, 2, 2 ]]`，这里的Shape为(1,7)，1是batch size, 7是句子长度。而向量中的0表示开始(`<bos>`)，1表示结束(`<eos>`)，2表示填充(`<pad>`)。这里我假设句子长度固定为7，所以需要填充两个字符。\n",
    "2. 当我们将文字转换为向量后，就会经过Embedding层对向量进行编码，我们这里将一个字符编码成128维的向量，而这个值也对应模型中的参数`d_model`(dimension of model)。而对向量编码一般使用参数可学习的`nn.Embedding`来实现。具体可参考[该篇文章](https://blog.csdn.net/zhaohongfei_358/article/details/122809709)\n",
    "3. 接下来我们使用Position Encoding为我们的Embedding增加位置成分。这里我假设位置1就是加0.1，位置2加0.2，依次类推\n",
    "4. 在完成位置编码后，就会正式进入Encoder层，我们有$X$个Encoder层，所以一层一层计算，最终得到Encoder的输出Memory，这个Memory的Shape与输入是一致的。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "BD7OY789VmQ7"
   },
   "id": "BD7OY789VmQ7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来我们来看Decoder的执行过程，如图所示：\n",
    "\n",
    "<img src=\"./images/decoder.png\" width=\"500\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "MGRnDA6QVmQ7"
   },
   "id": "MGRnDA6QVmQ7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Decoder的执行过程与Encoder类似，区别主要有以下：\n",
    "\n",
    "1. **输入为目标句子**：Encoder的输入是原句子（即要被翻译的句子），而Decoder的输入是目标句子（即翻译后的句子）。在推理时，Decoder是一遍一遍的执行，每次的输入都是之前的所有输出，例如，第一次输入为`<bos>`，输出为`我`，则第二次输入为`<bos> 我`, 输出为`爱`，依次类推。而在训练时，是一次将目标句子全部送给Decoder，通过掩码（mask）的方式来得到和推理一个一个送同样的结果。\n",
    "2. 第二步与Encoder一致\n",
    "3. Decoder层的输入除了有上一层的输出外，还需要Encoder的输出Memory。在Encoder层的Attention层，query、key和value都是输入x（即前一层的输出）。而在Decoder的Attention层，query是x，但key和value都是memory.\n",
    "4. 在Decoder层输出结果后，需要经过Generator线性层进行最后的预测，其会输出一个概率分布根据这个概率分布找出最大值对应的index，然后查词典就可以了。这个就是个标准的分类网络。需要注意的一点是，在推理时，只需要拿Decoder输出的最后一个tensor送给Generator，得到一个词的概率分布，就像上图画的那样。而在训练时，需要将Decoder的所有输出送给Generator，然后得到每个词的预测结果。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "0BJdtwfoVmQ7"
   },
   "id": "0BJdtwfoVmQ7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在Decoder输出结果后，需要将结果送给Generator(Linear+Softmax)来预测下一个词(token)。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "CXk95GN4VmQ8"
   },
   "id": "CXk95GN4VmQ8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bfcee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.725923Z",
     "iopub.status.busy": "2022-05-02T01:25:18.724967Z",
     "iopub.status.idle": "2022-05-02T01:25:18.727616Z",
     "shell.execute_reply": "2022-05-02T01:25:18.726912Z"
    },
    "id": "85bfcee7",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder的输出会送到Generator中做最后的预测。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        d_model: dimension of model. 这个值其实就是word embedding的维度。\n",
    "                 例如，你把一个词编码成512维的向量，那么d_model就是512\n",
    "        vocab: 词典的大小。\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x为Decoder的输出，例如x.shape为(1, 7, 128)，其中1为batch size, 7为句子长度，128为词向量的维度\n",
    "\n",
    "        这里使用的是log_softmax而非softmax，效果应该是一样的。\n",
    "        据说log_softmax能够解决函数overflow和underflow，加快运算速度，提高数据稳定性。\n",
    "        可以参考：https://www.zhihu.com/question/358069078/answer/912691444\n",
    "        该作者说可以把softmax都可以尝试换成log_softmax\n",
    "        \"\"\"\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef04378",
   "metadata": {
    "id": "0ef04378"
   },
   "source": [
    "## Encoder and Decoder 的堆叠\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Encoder是由多个相同的EncoderLayer堆叠而成的。原图中只画了一个EncoderLayer，然后旁边写了个 $N_x$ 。这个Nx就表示Encoder是由几个EncoderLayer堆叠而成。默认$N=6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367c79f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.733957Z",
     "iopub.status.busy": "2022-05-02T01:25:18.733114Z",
     "iopub.status.idle": "2022-05-02T01:25:18.735146Z",
     "shell.execute_reply": "2022-05-02T01:25:18.735851Z"
    },
    "id": "0367c79f"
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    该方法负责产生多个相同的层。\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d643018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.744483Z",
     "iopub.status.busy": "2022-05-02T01:25:18.743617Z",
     "iopub.status.idle": "2022-05-02T01:25:18.745891Z",
     "shell.execute_reply": "2022-05-02T01:25:18.746578Z"
    },
    "id": "1d643018"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder的核心部分就是多个相同的EncoderLayer堆叠而成。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        \"\"\"\n",
    "        初始化传入两个参数：\n",
    "        layer: 要堆叠的层，对应下面的EncoderLayer类\n",
    "        N: 堆叠多少次\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # 将Layer克隆N份\n",
    "        self.layers = clones(layer, N)\n",
    "        # LayerNorm层就是BatchNorm。也就是对应Transformer中\n",
    "        # “Add & Norm”中的“Norm”部分\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        x: 进行过Embedding和位置编码后的输入inputs。Shape为(batch_size, 词数，词向量维度)\n",
    "           例如(1, 7, 128)，batch_size为1，7个词，每个词128维\n",
    "        mask: src_mask，请参考EncoderDecoder.forward中的src_mask注释\n",
    "        \"\"\"\n",
    "\n",
    "        # 一层一层的执行，前一个EncoderLayer的输出作为下一层的输入\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        # 你可能会有疑问，为什么这里会有一个self.norm(x)，\n",
    "        # 这个疑问会在后面的`SublayerConnection`中给出解释\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f730d85",
   "metadata": {
    "id": "9f730d85"
   },
   "source": [
    "下面是LayerNorm的代码实现，这个应该算是`torch.nn.BatchNorm2d`一个简单的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb56a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.756189Z",
     "iopub.status.busy": "2022-05-02T01:25:18.755315Z",
     "iopub.status.idle": "2022-05-02T01:25:18.757924Z",
     "shell.execute_reply": "2022-05-02T01:25:18.758590Z"
    },
    "id": "1eb56a59"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Norm层，其实该层的作用就是BatchNorm。与`torch.nn.BatchNorm2d`的作用一致。\n",
    "    torch.nn.BatchNorm2d的官方文档地址：https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "\n",
    "    该LayerNorm就对应原图中 “Add & Norm”中“Norm”的部分\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        \"\"\"\n",
    "        features: int类型，含义为特征数。也就是一个词向量的维度，例如128。该值一般和d_model一致。\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        \"\"\"\n",
    "        这两个参数是BatchNorm的参数，a_2对应gamma(γ), b_2对应beta(β)。\n",
    "        而nn.Parameter的作用就是将这个两个参数作为模型参数，之后要进行梯度下降。\n",
    "        \"\"\"\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        # epsilon，一个很小的数，防止分母为0\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x： 为Attention层或者Feed Forward层的输出。Shape和Encoder的输入一样。（其实整个过程中，x的shape都不会发生改变）。\n",
    "            例如，x的shape为(1, 7, 128)，即batch_size为1，7个单词，每个单词是128维度的向量。\n",
    "        \"\"\"\n",
    "\n",
    "        # 按最后一个维度求均值。mean的shape为 (1, 7, 1)\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        # 按最后一个维度求方差。std的shape为 (1, 7, 1)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # 进行归一化，详情可查阅BatchNorm相关资料。\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1edc4",
   "metadata": {
    "id": "c9c1edc4"
   },
   "source": [
    "接下来是“Add & Norm”部分，在该代码中称为`SublayerConnection`，因为“Add & Norm”就是两个子层之间的残差连接。关于残差连接，可以参考ResNet网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f34584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.767044Z",
     "iopub.status.busy": "2022-05-02T01:25:18.766174Z",
     "iopub.status.idle": "2022-05-02T01:25:18.768689Z",
     "shell.execute_reply": "2022-05-02T01:25:18.769383Z"
    },
    "id": "a7f34584"
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    在一个Norm层后跟一个残差连接。\n",
    "    注意，为了代码简洁，Norm层是在最前面，而不是在最后面\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        \"\"\"\n",
    "        这里的size就是d_model。也就是词向量的维度\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        # BatchNorm层\n",
    "        self.norm = LayerNorm(size)\n",
    "        # BatchNorm的dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        x：本层的输入，前一层的输出。\n",
    "        sublayer: 这个Sublayer就是Attention层或Feed ForWard层。\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "在上面的代码中，有这么一句注释：**Norm层是在最前面，而不是在最后面**。这句话怎么理解呢？\n",
    "\n",
    "在Transformer原图中，Encoder架构是这样：`Attention` -> `Add & Norm` -> `Feed Forward` -> `Add & Norm`。\n",
    "\n",
    "所以我们会下意识地认为代码也是按照这个顺序实现的，但实际代码为了简单，是按照下图这个顺序实现的：\n",
    "\n",
    "<img src=\"./images/encoder2.png\" width=\"500\">\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Gfzv376QVmQ9"
   },
   "id": "Gfzv376QVmQ9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "从上图可以看到，Norm是在是放在最前面的，而不是最后面，所以在`SublayerConnection`中的代码是：\n",
    "\n",
    "```\n",
    "return x + self.dropout(sublayer(self.norm(x)))\n",
    "```\n",
    "\n",
    "而不是\n",
    "\n",
    "```\n",
    "return self.norm(x + self.dropout(sublayer(x)))\n",
    "```\n",
    "\n",
    "同时我们也能发现另外两点：\n",
    "\n",
    "1. 在向量经过Position Encoding后到进入Encoder前是有一个Norm动作的。\n",
    "2. 最后一个EncoderLayer层Add后并没有Norm，所以要补一个，这也就是为什么在Encoder类的返回是`return self.norm(x)`而不是直接`return x`。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "olBmQkqqVmQ9"
   },
   "id": "olBmQkqqVmQ9"
  },
  {
   "cell_type": "markdown",
   "id": "e1d6e20d",
   "metadata": {
    "id": "e1d6e20d"
   },
   "source": [
    "Encoder的每一层都有两个子层(sub-layers)，第一个是多头注意力机制（multi-head\n",
    "self-attention mechanism），第二个是一个简单的全连接前馈神经网络（Feed Forward）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db97336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.779893Z",
     "iopub.status.busy": "2022-05-02T01:25:18.778804Z",
     "iopub.status.idle": "2022-05-02T01:25:18.780994Z",
     "shell.execute_reply": "2022-05-02T01:25:18.781710Z"
    },
    "id": "3db97336"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder由一个Attention层和一个前馈网络组成\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        \"\"\"\n",
    "        size: 就是d_model，也是词向量的维度。\n",
    "        self_attn: MultiHead Self-Attention模型\n",
    "        feed_forward: 前馈神经网络\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        # 克隆两个SublayerConnection，第一个给Attention用，第二个给Feed Forward用\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        该方法对应原图中Encoder的部分\n",
    "        \"\"\"\n",
    "        # 首先执行Attention层的计算\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # 接着执行FeedForward层的计算\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90d6ee",
   "metadata": {
    "id": "9c90d6ee"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "Decoder和Encoder类似，是由 $N$ 个相同的DecoderLayer层堆积而成的。默认 $N=6$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe403d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.790602Z",
     "iopub.status.busy": "2022-05-02T01:25:18.789754Z",
     "iopub.status.idle": "2022-05-02T01:25:18.791985Z",
     "shell.execute_reply": "2022-05-02T01:25:18.792756Z"
    },
    "id": "4dbe403d"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        \"\"\"\n",
    "        layer: 要堆叠的层，对应下面的DecoderLayer类\n",
    "        N: 堆叠多少次\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        # 克隆出N个DecoderLayer\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        x: 进行过Embedding和位置编码后的“输入outputs”。Shape为(batch_size, 词数，词向量维度)\n",
    "           例如(1, 7, 128)，batch_size为1，7个词，每个词128维\n",
    "           在预测时，x的词数会不断变化，x的shape第一次为(1, 1, 128)，第二次为(1, 2, 128)，以此类推。\n",
    "\n",
    "        其他参数注释请参考上面的EncoderDecoder类\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202cebd",
   "metadata": {
    "id": "c202cebd"
   },
   "source": [
    "除了Encoder的两个sublayer外，decoder还在中间插入了第三个子层（就是Decoder中Feed Forward和Masked Multi-head Attention中间的那个Multi-Head Attention），该子层也是一个Attention层，只不过这次Attention是针对Encoder的输出Memory和Decoder的第一个Attention的输出。与Encoder类似，每个子层也都做了“Add & Norm”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1df6b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.803099Z",
     "iopub.status.busy": "2022-05-02T01:25:18.796132Z",
     "iopub.status.idle": "2022-05-02T01:25:18.806479Z",
     "shell.execute_reply": "2022-05-02T01:25:18.805667Z"
    },
    "id": "3b1df6b1"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder由三部分组成：\n",
    "    1. self_attn: 对应图中的Masked Multi-head Attention，负责对Outputs做Attention。\n",
    "    2. src_attn：对应图中Decoder中间的Multi-head Attention，负责对前面Attention的输出\n",
    "                 和Encoder的输出做Attention\n",
    "    3. feed_forward：对应Decoder的Feed Forward\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        \"\"\"\n",
    "        size: d_model，也就是词向量的维度。\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        # 因为Decoder有三个sublayer，所以拷贝3份。\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"对应原图中Decoder的部分\"\n",
    "        m = memory\n",
    "        # DecoderLayer的第一个Attention，对应的Mask MultiHead Attention\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # DecoderLayer的第二个Attention（中间那个），attention的key和value使用的是memory\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        # DecoderLayer最后的Feed Forward层。\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6852ba5",
   "metadata": {
    "id": "a6852ba5"
   },
   "source": [
    "Transformer的Attention层还有一个mask机制，主要是为了防止词向量注意一些不该注意的内容，例如：对于Encoder来说，不应该注意`<pad>`的部分，因为这不属于句子成分。对于Decoder来说，前面的词不应该注意后面的词，同时，也不能注意`<pad>`的部分。\n",
    "\n",
    "关于注意力机制和Mask，可参考[该篇文章](https://blog.csdn.net/zhaohongfei_358/article/details/122861751)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1b467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.813309Z",
     "iopub.status.busy": "2022-05-02T01:25:18.812387Z",
     "iopub.status.idle": "2022-05-02T01:25:18.815171Z",
     "shell.execute_reply": "2022-05-02T01:25:18.814519Z"
    },
    "id": "1fe1b467"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    生成一个大小为 size x size 的方阵，该方阵对角线与左下全为True，右上全为False。\n",
    "    例如，size为5时，则结果为：\n",
    "    [[[ True, False, False, False, False],\n",
    "      [ True,  True, False, False, False],\n",
    "      [ True,  True,  True, False, False],\n",
    "      [ True,  True,  True,  True, False],\n",
    "      [ True,  True,  True,  True,  True]]]\n",
    "\n",
    "    该方法在会在构建tgt的mask时使用。\n",
    "    \"\"\"\n",
    "\n",
    "    # 生成一个Shape为(1, size, size)的矩阵，这里要在\n",
    "    # 前面加个1是为了和tgt的tensor维度保持一致，\n",
    "    # 因为tgt最前面是batch_size\n",
    "    attn_shape = (1, size, size)\n",
    "\n",
    "    \"\"\"\n",
    "    生成一个左上角全为1的方阵，例如，当attn_shape为5时，结果为：\n",
    "       [[[0., 1., 1., 1., 1.],\n",
    "         [0., 0., 1., 1., 1.],\n",
    "         [0., 0., 0., 1., 1.],\n",
    "         [0., 0., 0., 0., 1.],\n",
    "         [0., 0., 0., 0., 0.]]]\n",
    "    \"\"\"\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "\n",
    "    # 将 0 全部变为True, 1变为False\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946c508",
   "metadata": {
    "id": "4946c508",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Attention\n",
    "\n",
    "\n",
    "Attention机制比较复杂，这里只是做简单的说明，如果感兴趣，可以参考[该文章](https://blog.csdn.net/zhaohongfei_358/article/details/122861751)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe740ac",
   "metadata": {
    "id": "4fe740ac"
   },
   "source": [
    "Self-Attention如果只看公式，其实也简单，就是执行下面这个公式即可：\n",
    "\n",
    "$$\n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "如果用图表示该公式的话，则为：\n",
    "\n",
    "![](./images/ModalNet-19.png)\n",
    "\n",
    "大部分对上述的疑问可能就是Q, K, V是怎么来的，$d_k$ 又是什么。其实是这样，在Self-Attention层会训练三个矩阵：$W^q, W^k和W^v$，然后用这三个矩阵乘以输入即可，即：\n",
    "\n",
    "$$\n",
    "Q = W^q I \\\\\n",
    "K = W^k I \\\\\n",
    "V = W^v I \\\\\n",
    "$$\n",
    "\n",
    "这里的I就是EncoderLayer层的输入。而 $d_k$ 就是`d_model`的值，例如128。（如果是MultiHead Attention，则`d_k = d_model / h`，h是head数量。所以这也是为什么d_model必须要能整除head数）\n",
    "\n",
    "通常我们定义这三个$W$矩阵是通过`nn.Linear`。如果你对`nn.Linear`的本质不太清楚，可以参考[该文章](https://blog.csdn.net/zhaohongfei_358/article/details/122797190)\n",
    "\n",
    "> 这里的三个$W$矩阵通常都是方阵，而Q,K,V的shape与输入I的shape一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92d7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.251181Z",
     "iopub.status.busy": "2022-05-02T01:25:19.250905Z",
     "iopub.status.idle": "2022-05-02T01:25:19.253190Z",
     "shell.execute_reply": "2022-05-02T01:25:19.253437Z"
    },
    "id": "6c92d7c5",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    计算Attention的结果。\n",
    "    这里其实传入的是Q,K,V，而Q,K,V的计算是放在模型中的，请参考后续的MultiHeadedAttention类。\n",
    "\n",
    "    这里的Q,K,V有两种Shape，如果是Self-Attention，Shape为(batch, 词数, d_model)，\n",
    "                           例如(1, 7, 128)，即batch_size为1，一句7个单词，每个单词128维\n",
    "\n",
    "                           但如果是Multi-Head Attention，则Shape为(batch, head数, 词数，d_model/head数)，\n",
    "                           例如(1, 8, 7, 16)，即Batch_size为1，8个head，一句7个单词，128/8=16。\n",
    "                           这样其实也能看出来，所谓的MultiHead其实就是将128拆开了。\n",
    "\n",
    "                           在Transformer中，由于使用的是MultiHead Attention，所以Q,K,V的Shape只会是第二种。\n",
    "\n",
    "    mask： mask有两种，一种是src_mask，另一种是tgt_mask。\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取d_model的值。之所以这样可以获取，是因为query和输入的shape相同，\n",
    "    # 若为Self-Attention，则最后一维都是词向量的维度，也就是d_model的值。\n",
    "    # 若为MultiHead Attention，则最后一维是 d_model / h，h为head数\n",
    "    d_k = query.size(-1)\n",
    "    # 执行QK^T / √d_k\n",
    "    # 这里scores是一个方阵, shape为(batch_size, head数，词数，词数)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    \"\"\"\n",
    "    mask有两种：\n",
    "    第一种为： src_mask。Shape为(batch_size, 1, 1, 词数)。例如：(1, 1, 1, 10)。\n",
    "              [[[1., 1., 1., 1., 1., 1., 1., 0., 0., 0.]]]，表示第一个句子的\n",
    "              前7个是单词（包括bos、eos和unk），后面3个是填充。而mask就是要对\n",
    "              后面三个进行mask。\n",
    "              例如，mask前，score的shape为(1, 2, 10, 10)，2个head，10个词\n",
    "              score在mask前为：\n",
    "              [[[[x1, x2, ..., x7, x8, x9, x10],\n",
    "                 ...\n",
    "                 [x1, x2, ..., x7, x8, x9, x10]]\n",
    "                 [x1, x2, ..., x7, x8, x9, x10],\n",
    "                 ...\n",
    "                 [x1, x2, ..., x7, x8, x9, x10]]]]\n",
    "             score在mask后为：\n",
    "              [[[[x1, x2, ..., x7, -1e9, -1e9, -1e9],\n",
    "                 ...\n",
    "                 [x1, x2, ..., x7, -1e9, -1e9, -1e9]]\n",
    "                 [x1, x2, ..., x7, -1e9, -1e9, -1e9],\n",
    "                 ...\n",
    "                 [x1, x2, ..., x7, -1e9, -1e9, -1e9]]]]\n",
    "\n",
    "    第二种为：tgt_mask。Shape为(batch_size, 1, 词数, 词数)。例如：(1, 1, 10, 10)。例如：\n",
    "             [[[1, 0, 0, 0, 0],\n",
    "               [1, 1, 0, 0, 0],\n",
    "               [1, 1, 1, 0, 0],\n",
    "               [1, 1, 1, 1, 0],\n",
    "               [1, 1, 1, 1, 1]]\n",
    "             即对角线的右上面都盖住。具体为什么这样可以参考链接：https://blog.csdn.net/zhaohongfei_358/article/details/125858248\n",
    "             score在mask前为：\n",
    "             [[[[x1, x2, ..., x7, x8, x9, x10],\n",
    "                 ...\n",
    "                 [x1, x2, ..., x7, x8, x9, x10]]\n",
    "                 [x1, x2, ..., x7, x8, x9, x10],\n",
    "                 ...\n",
    "                 [x1, x2, ..., x7, x8, x9, x10]]]]\n",
    "            score在mask后为：\n",
    "             [[[[x1, 1e9, ..., 1e9, -1e9, -1e9, -1e9],\n",
    "                 ...\n",
    "                [x1, x2, ..., 1e9, -1e9, -1e9, -1e9]]\n",
    "                [x1, x2, ..., 1e9, -1e9, -1e9, -1e9],\n",
    "                ...\n",
    "                [x1, x2, ..., x7, x8, x9, x10]]]]\n",
    "\n",
    "    两者比较：src_mask的shape为(batch_size, 1, 1, 词数)，而tgt_mask的shape为(batch_size, 1, 词数, 词数)\n",
    "             这是因为src要对后面非句子的词整个mask，所以只需要在最后一维做就行了。\n",
    "             而tgt需要斜着进行mask，所以需要一个方阵来进行。\n",
    "    \"\"\"\n",
    "    if mask is not None:\n",
    "        # 这里的填充值并不是0，而是-1e9，原因是后续还要进行softmax，\n",
    "        # 而softmax会让负无穷变为0。\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # 执行公式中的Softmax\n",
    "    # 这里的p_attn是一个方阵\n",
    "    # 若是Self Attention，则shape为(batch, 词数, 次数)，例如(1, 7, 7)\n",
    "    # 若是MultiHead Attention，则shape为(batch, head数, 词数，词数)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    # 最后再乘以 V。\n",
    "    # 对于Self Attention来说，结果Shape为(batch, 词数, d_model)，这也就是最终的结果了。\n",
    "    # 但对于MultiHead Attention来说，结果Shape为(batch, head数, 词数，d_model/head数)\n",
    "    # 而这不是最终结果，后续还要将head合并，变为(batch, 词数, d_model)。不过这是MultiHeadAttention\n",
    "    # 该做的事情。\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "ZsMGdksTVmQ-"
   },
   "id": "ZsMGdksTVmQ-"
  },
  {
   "cell_type": "markdown",
   "id": "795454ef",
   "metadata": {
    "id": "795454ef"
   },
   "source": [
    "Multi Attention建议参考[该文章](https://blog.csdn.net/zhaohongfei_358/article/details/122861751)。\n",
    "\n",
    "Multi Attention简单点说，就是将Q, K, V的最后一个维度进行拆分，拆成了多个head，然后再带入attention函数进行计算，最终再将多个head融合到一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        h: head的数量\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        # 定义W^q, W^k, W^v和W^o矩阵。\n",
    "        # 如果你不知道为什么用nn.Linear定义矩阵，可以参考该文章：\n",
    "        # https://blog.csdn.net/zhaohongfei_358/article/details/122797190\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query, key, value: 这里的这三个变量名容易对人产生误导，\n",
    "        它指的并不是公式中经过W^q, W^k, W^v计算后的Q, K, V，\n",
    "        而是要跟 W^q, W^k, W^v 计算的输入。\n",
    "        对于Encoder的Attention层来说，这里的query,key,value全都是输入x，\n",
    "        从EncoderLayer类中的`self.self_attn(x, x, x, mask)`这段代码也\n",
    "        可以看出来。\n",
    "        对于Decoder最前面那个Mask Attention，query,key,value也全都是输入x。\n",
    "        但对于Decoder中间的那个Attention就不一样了，它是将Encoder的输出memory\n",
    "        作为key和value，将输入x作为query。通过DecoderLayer中的代码\n",
    "        `self.src_attn(x, m, m, src_mask)`可看出来。\n",
    "        \"\"\"\n",
    "\n",
    "        if mask is not None:\n",
    "            # 原本mask和query的Tensor维度都是3，即len(mask.size())为3\n",
    "            # 但是由于是多头注意力机制，所以query的维度会变成4，也\n",
    "            # 就是在第2个维度插入了head，变为了(batch, head数, 词数，d_model/head数)。\n",
    "            # mask为了和query的维度保持一致，所以也要将自己扩展成4维。\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # 获取Batch Size\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        \"\"\"\n",
    "        1. 求出Q, K, V，这里是求MultiHead的Q,K,V，所以Shape为(batch, head数, 词数，d_model/head数)\n",
    "            1.1 首先，通过定义的W^q,W^k,W^v求出SelfAttention的Q,K,V，此时Q,K,V的Shape为(batch, 词数, d_model)\n",
    "                对应代码为 `linear(x)`\n",
    "            1.2 分成多头，即将Shape由(batch, 词数, d_model)变为(batch, 词数, head数，d_model/head数)。\n",
    "                对应代码为 `view(nbatches, -1, self.h, self.d_k)`\n",
    "            1.3 最终交换“词数”和“head数”这两个维度，将head数放在前面，最终shape变为(batch, head数, 词数，d_model/head数)。\n",
    "                对应代码为 `transpose(1, 2)`\n",
    "        \"\"\"\n",
    "        query, key, value = [\n",
    "            linear(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for linear, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        \"\"\"\n",
    "        2. 求出Q,K,V后，通过attention函数计算出Attention结果，\n",
    "           这里x的shape为(batch, head数, 词数，d_model/head数)\n",
    "           self.attn的shape为(batch, head数, 词数，词数)\n",
    "        \"\"\"\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        3. 将多个head再合并起来，即将x的shape由(batch, head数, 词数，d_model/head数)\n",
    "           再变为 (batch, 词数，d_model)\n",
    "           3.1 首先，交换“head数”和“词数”，这两个维度，结果为(batch, 词数, head数, d_model/head数)\n",
    "               对应代码为：`x.transpose(1, 2).contiguous()`\n",
    "           3.2 然后将“head数”和“d_model/head数”这两个维度合并，结果为(batch, 词数，d_model)\n",
    "        \"\"\"\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "\n",
    "        # 最终通过W^o矩阵再执行一次线性变换，得到最终结果。\n",
    "        return self.linears[-1](x)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "kqLhz6_kVmQ-"
   },
   "id": "kqLhz6_kVmQ-"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Position-wise 前馈神经网络\n",
    "\n",
    "除了Attention子层外，Encoder和Decoder的每一个层还包含一个全连接的前馈网络，该网络分别且相同地应用于每个位置。该前馈网络包括两个线性变换，并在第一个的最后使用ReLU激活函数。\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
    "\n",
    "虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。描述这种情况的另一种方式是两个内核大小为1的卷积。输入和输出的维度是$d_{\\text{model}}=512$，内层的维度$d_{ff}=2048$。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "D6O3rwhyVmQ-"
   },
   "id": "D6O3rwhyVmQ-"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf547802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.274902Z",
     "iopub.status.busy": "2022-05-02T01:25:19.274161Z",
     "iopub.status.idle": "2022-05-02T01:25:19.277708Z",
     "shell.execute_reply": "2022-05-02T01:25:19.278473Z"
    },
    "id": "cf547802"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        d_ff: 隐层的神经元数量。\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 公式中的max(0, xW+b)其实就是ReLU的公式\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embeddings and Softmax\n",
    "\n",
    "与其他的序列转化模型相同，我们也是用可学习的Embeddings来将input和output的token和转换为$d_{\\text{model}}$维的向量。\n",
    "\n",
    "我们也使用常用的线性层和Softmax函数来对decoder的输出做预测，预测下一个token的概率。在我们的模型中，两个Embedding层使用相同的权重。在Embedding层，会乘以权重$\\sqrt{d_{\\text{model}}}$\n",
    "\n",
    "关于`nn.Embedding`的使用，可参考[该文章](https://blog.csdn.net/zhaohongfei_358/article/details/122809709)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "w22nnQ1JVmQ_"
   },
   "id": "w22nnQ1JVmQ_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851b029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.286884Z",
     "iopub.status.busy": "2022-05-02T01:25:19.285947Z",
     "iopub.status.idle": "2022-05-02T01:25:19.288160Z",
     "shell.execute_reply": "2022-05-02T01:25:19.288878Z"
    },
    "id": "f851b029"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        d_model: 词向量的维度\n",
    "        vocab: 词典的大小。\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "关于为什么Embedding层要乘以权重$\\sqrt{d_{\\text{model}}}$，可以参考这个[知乎问题](https://www.zhihu.com/question/415263284)中@王四喜和@Quokka的回答。这里我只做一个直观的说明。\n",
    "\n",
    "由于模型使用了`nn.init.xavier_uniform_`的方式进行初始化，会导致Embedding的结果长这样（`self.lut(x)`的结果）：\n",
    "\n",
    "\n",
    "```\n",
    "[[[ 0.0052, -0.0021,  0.0071,  ..., -0.0161, -0.0092, -0.0077],\n",
    "  [-0.0170,  0.0148, -0.0011,  ...,  0.0118, -0.0128,  0.0143],\n",
    "  [-0.0137,  0.0146,  0.0196,  ..., -0.0194, -0.0049, -0.0072],\n",
    "  ...\n",
    "```\n",
    "\n",
    "可以看到，都是0.0x开头的。若乘以$\\sqrt{d_{\\text{model}}}$，结果就变为了：\n",
    "\n",
    "```\n",
    "[[[ 0.0593, -0.0239,  0.0808,  ..., -0.1818, -0.1036, -0.0875],\n",
    "  [-0.1924,  0.1677, -0.0127,  ...,  0.1332, -0.1452,  0.1619],\n",
    "  [-0.1553,  0.1656,  0.2222,  ..., -0.2198, -0.0554, -0.0815],\n",
    "  ...\n",
    "```\n",
    "\n",
    "所以乘以$\\sqrt{d_{\\text{model}}}$其实就是起到了一个标准化的作用。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "HAwgVp4AVmQ_"
   },
   "id": "HAwgVp4AVmQ_"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 位置编码（Position Encoding）\n",
    "\n",
    "因为Transformer模型不像RNN那样存在循环卷积，为了让模型可以利用输入序列的顺序，我们就必须往序列中注入一些相对位置和绝对位置的信息。因此，我们在input embeddings后增加了“positional encodings”. Positional Encodings与$d_{\\text{model}}$有相同的维度，这样他们就可以相加了。\n",
    "\n",
    "在该工作中，我们使用了不同频率的sine和cosine函数：\n",
    "\n",
    "$$PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "$$PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "其中$pos$是位置，$i$是维度。也就是说，位置编码的每个维度都对应着一个正弦曲线(sinusoid)。波长形成$2\\pi$ 到 $10000\\cdot 2\\pi$的几何级数。我们选择了这个函数，因为我们假设它允许模型容易地学习相对位置，因为对于任何固定偏移$k$, $PE_{pos+k}$可以表示为$PE_{pos}$的线性函数。\n",
    "\n",
    "除此之外，我们还在embedding与positional encoding的sum结果上应用了dropout。默认dropout=0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "JUxWJpMoVmQ_"
   },
   "id": "JUxWJpMoVmQ_"
  },
  {
   "cell_type": "markdown",
   "source": [
    "上面这段原论文的解释看起来有些晦涩难懂，可以参考[该篇文章](https://zhuanlan.zhihu.com/p/166244505)。\n",
    "\n",
    "总之就是，我们对第pos个单词的第i个维度，就是加上$PE(pos, 2i)$即可。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Tcb5KQhKVmQ_"
   },
   "id": "Tcb5KQhKVmQ_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfacc553",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.300508Z",
     "iopub.status.busy": "2022-05-02T01:25:19.299554Z",
     "iopub.status.idle": "2022-05-02T01:25:19.301927Z",
     "shell.execute_reply": "2022-05-02T01:25:19.302701Z"
    },
    "id": "dfacc553"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de678b",
   "metadata": {
    "id": "04de678b"
   },
   "source": [
    "> 在上面的代码计算`div_term`时，并没有直接使用 $pos / 10000^{2i/d_{\\text{model}}}$ 公式，而是使用它的变种 $e^{-2i\\ln(10000) / d_{model}}$，我认为原因可能是因为这样计算效率较高。\n",
    "> 我测试了使用原始公式的代码`1 / torch.pow(10000, torch.arange(0., d_model, 2) / d_model)`，发现结果和变种一样，但速度却慢了一倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aef3a8",
   "metadata": {
    "id": "74aef3a8"
   },
   "source": [
    "## 完整模型\n",
    "\n",
    "这里我们定义一个函数来构建完整模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e5768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.633281Z",
     "iopub.status.busy": "2022-05-02T01:25:19.632424Z",
     "iopub.status.idle": "2022-05-02T01:25:19.634909Z",
     "shell.execute_reply": "2022-05-02T01:25:19.634216Z"
    },
    "id": "460e5768"
   },
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper: Construct a model from hyperparameters.\n",
    "\n",
    "    src_vocab：原词典的大小。假设你要做英译汉，那么src_vocab就是英文词典的大小\n",
    "    tgt_vocab: 目标词典的大小。\n",
    "    N: EncoderLayer和DecoderLayer的数量\n",
    "    d_model: 词向量的大小。\n",
    "    d_ff: 模型中Feed Forward层中隐层神经元的数量\n",
    "    h: MultiHead中head的数量\n",
    "    \"\"\"\n",
    "    # 用于将模型深度拷贝一份（相当于全新的new一个）\n",
    "    c = copy.deepcopy\n",
    "    # 1. 构建多头注意力机制\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    # 2. 构建前馈神经网络\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    # 3. 构建位置编码\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    # 4. 构建Transformer模型\n",
    "    model = EncoderDecoder(\n",
    "        # Encoder层。\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        # Decoder层，其包含两个Attention层，复制两份\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        # inputs的编码器和位置编码\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        # outputs的编码器和位置编码\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        # Decoder最后的Linear层和Softmax，用于预测下一个token\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    # 使用xavier均匀分布的方式进行模型参数初始化\n",
    "    # 关于为什么要用该初始化方式，我并没有在网上\n",
    "    # 找到很好的解释。\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 推理(Inference)\n",
    "\n",
    "我们这里使用前向传递来测试一下我们的模型。我们会尝试使用Transformer来记住input。因为模型没训练过，所以输出是随机的。不过在下一章，我们将会构建训练函数来尝试训练我们的模型，让他记住从1到10这几个数字。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "61hvGuKuVmQ_"
   },
   "id": "61hvGuKuVmQ_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316fb1d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:19.641806Z",
     "iopub.status.busy": "2022-05-02T01:25:19.640944Z",
     "iopub.status.idle": "2022-05-02T01:25:20.885368Z",
     "shell.execute_reply": "2022-05-02T01:25:20.885607Z"
    },
    "id": "316fb1d2"
   },
   "outputs": [],
   "source": [
    "def inference_test():\n",
    "    \"\"\"\n",
    "    测试模型\n",
    "    \"\"\"\n",
    "    # 构架测试模型，原词典和目标词典大小都为11，\n",
    "    # EncoderLayer和DecoderLayer的数量为2\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    test_model.eval()\n",
    "    # 定义inputs, shape为(1, 10)，即一个句子，该句子10个单词。\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    # 定义src_mask，即所有的词都是有效的，没有填充词\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "    # 将输入送给encoder，获取memory\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "\n",
    "    # 初始化ys为[[0]]，用于保存预测结果，其中0表示'<bos>'\n",
    "    ys = torch.zeros(1, 1).type_as(src)\n",
    "\n",
    "    # 循环调用decoder，一个个的进行预测。例如：假设我们要将“I love you”翻译成\n",
    "    # “我爱你”，则第一次的`ys`为(<bos>)，然后输出为“I”。然后第二次`ys`为(<bos>, I)\n",
    "    # 输出为\"love\"，依次类推，直到decoder输出“<eos>”或达到句子长度。\n",
    "    for i in range(9):\n",
    "        # 将encoder的输出memory和之前Decoder的所有输出作为参数，让Decoder来预测下一个token\n",
    "        out = test_model.decode(\n",
    "            # ys就是Decoder之前的所有输出\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        # 将Decoder的输出送给generator进行预测。这里只取最后一个词的输出进行预测。\n",
    "        # 因为你传的tgt的词数是变化的，第一次是(<bos>)，第二次是(<bos>, I)\n",
    "        # 所以你的out的维度也是变化的，变化的就是(batch_size, 词数，词向量)中词数这个维度\n",
    "        # 既然只能取一个，那当然是最后一个词最合适。\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        # 取出数值最大的那个，它的index在词典中对应的词就是预测结果\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        # 取出预测结果\n",
    "        next_word = next_word.data[0]\n",
    "        # 将这一次的预测结果和之前的拼到一块，作为之后Decoder的输入\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n",
    "\n",
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859848fd",
   "metadata": {
    "id": "859848fd"
   },
   "source": [
    "# Part 2: 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acd58b",
   "metadata": {
    "id": "16acd58b"
   },
   "source": [
    "本章描述模型的训练机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "在正式进入训练模型之前，需要先定义一些工具类和方法。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "kzTCiEW0VmRA"
   },
   "id": "kzTCiEW0VmRA"
  },
  {
   "cell_type": "markdown",
   "id": "918f5ddd",
   "metadata": {
    "id": "918f5ddd"
   },
   "source": [
    "## Batches and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"\n",
    "    定义一个Batch，来存放一个batch的src，tgt，src_mask等对象。\n",
    "    方便后续的取用\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n",
    "        \"\"\"\n",
    "        src: 和EncoderDecoder#forward中的那个src一致。\n",
    "             未进行word embedding的句子，例如`[[ 0, 5, 4, 6, 1, 2, 2 ]]`\n",
    "             上例shape为(1, 7)，即batch size为1，句子大小为7。其中0为bos，\n",
    "             1为eos, 2为pad\n",
    "\n",
    "        tgt: 和src类似。是目标句子。\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "\n",
    "        \"\"\"\n",
    "        构造src_mask：就是将src中pad的部分给盖住，因为这些不属于句子成分，不应该参与计算。\n",
    "                     例如，src为[[ 0, 5, 4, 6, 1, 2, 2 ]]，则src_mask则为：\n",
    "                     [[[ True, True, True, True, True, False, False ]]]。因为最后两个2(pad)\n",
    "                     不属于句子成分。（“<bos>”、“<eos>”和“<unk>”是要算作句子成分的）\n",
    "        这里unsqueeze一下是因为后续是要对Attention中的scores进行mask，而scores的len(shape)=3,\n",
    "        为了与scores保持一致，所以unsqueeze(-2)一下。具体可参考attention函数中的注释。\n",
    "        \"\"\"\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if tgt is not None:\n",
    "            \"\"\"\n",
    "            每个句子都去掉最后一个词。例如tgt的Shape为(16, 30)，\n",
    "            即batch_size为16，每个句子30个单词，执行该代码后为\n",
    "            (16, 29)。这么做的原因是：tgt存储的是Decoder的输入，\n",
    "            而Decoder的输入是不可能包含最后一个词的。例如，我们要\n",
    "            预测`<bos> 我 爱 你 <eos>`，第一次我们会给Decoder\n",
    "            传`<bos>`，第二次传`<bos> 我`，而最后一次会传\n",
    "            `<bos> 我 爱 你`。所以tgt不会出现最后一个词，所以要去掉。\n",
    "            \"\"\"\n",
    "            self.tgt = tgt[:, :-1]\n",
    "\n",
    "            \"\"\"\n",
    "            与上面差不多，去掉句子的第一个词，也就是“<bos>”\n",
    "            tgt_y 存储的是希望预测的结果，所以不需要'<bos>'\n",
    "            例如，传入encoder的是\"<bos> I love you <eos>\"，\n",
    "            初始传入decoder为\"<bos>\"，则我们想最终能“一个个”\n",
    "            的预测出“我 爱 你 <eos>”，所以要把<bos>去掉，因为\n",
    "            其不是我们想要预测的token。\n",
    "            \"\"\"\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "\n",
    "            \"\"\"\n",
    "            构造tgt_mask：tgt_mask与src_mask略有不同，除了需要盖住pad部分，还需要\n",
    "                          将对角线右上的也都盖住，具体原因可参考：https://blog.csdn.net/zhaohongfei_358/article/details/125858248\n",
    "                          例如：[[ 0, 5, 4, 6, 1, 2, 2 ]]，则tgt_mask则为：\n",
    "                          [[[ True, False, False, False, False, False, False],\n",
    "                            [ True,  True, False, False, False, False, False],\n",
    "                            [ True,  True,  True, False, False, False, False],\n",
    "                            [ True,  True,  True,  True, False, False, False],\n",
    "                            [ True,  True,  True,  True,  True, False, False],\n",
    "                            [ True,  True,  True,  True,  True, False, False],\n",
    "                            [ True,  True,  True,  True,  True, False, False]]]\n",
    "            \"\"\"\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "\n",
    "            \"\"\"\n",
    "            此Batch的tgt_y的总token数量。值为一个数字，例如 tensor(266) 表示tgt_y有266个有意义的token\n",
    "            也就是出去<pad>部分的词的数量。保存这个是为了用于loss的正则化，后面loss部分会详细说明。\n",
    "            注意这里是tgt_y，而tgt_y去掉了“<bos>”，所以token数是不包含“<bos>”的。\n",
    "            \"\"\"\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad=2):\n",
    "        \"\"\"\n",
    "        生成tgt_mask\n",
    "        \"\"\"\n",
    "        # 首先生成非句子成分部分的mask\n",
    "        # 例如 [[ 0, 5, 4, 6, 1, 2, 2 ]] 的mask为 [[[ True, True, True, True, True, False, False ]]]\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "\n",
    "        \"\"\"\n",
    "        subsequent_mask用于获取阶梯式Mask。然后再和tgt_mask进行&操作。\n",
    "        例如：tgt_mask为[[[ True, True, False ]]]\n",
    "              subsequent_mask结果为：\n",
    "                        [[[ True, False, False ],\n",
    "                          [ True, True, False ],\n",
    "                          [ True, True, True ]]]\n",
    "              则tgt_mask & subsequent_mask结果为：\n",
    "                        [[[ True, False, False ],\n",
    "                          [ True, True, False ],\n",
    "                          [ True, True, False ]]]\n",
    "        \"\"\"\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SWyJr1FgVmRA"
   },
   "id": "SWyJr1FgVmRA"
  },
  {
   "cell_type": "markdown",
   "id": "934a7a4d",
   "metadata": {
    "id": "934a7a4d"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来我们定义一个通用的训练函数和训练状态类："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "58lQ_2nYVmRA"
   },
   "id": "58lQ_2nYVmRA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32298c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:20.895558Z",
     "iopub.status.busy": "2022-05-02T01:25:20.895196Z",
     "iopub.status.idle": "2022-05-02T01:25:20.896378Z",
     "shell.execute_reply": "2022-05-02T01:25:20.896666Z"
    },
    "id": "bd32298c"
   },
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"用于保存一些训练状态\"\"\"\n",
    "\n",
    "    # step的次数，但注意是一次loss.backward()算一次，或者说一个batch算一次\n",
    "    # 并不是一次optimizer.step()算一次。在后面的训练代码中，可能会累计多次loss\n",
    "    # 然后进行一次optimizer.step()\n",
    "    step: int = 0\n",
    "\n",
    "    # 参数更新的次数。这个才是optimizer.step()的次数\n",
    "    accum_step: int = 0\n",
    "\n",
    "    samples: int = 0  # 记录训练过的样本数量\n",
    "    tokens: int = 0  # 记录处理过的token数量（target的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"\n",
    "    进行一个epoch训练\n",
    "\n",
    "    data_iter: 可迭代对象，一次返回一个Batch对象\n",
    "    model: Transformer模型，EncoderDecoder类对象\n",
    "    loss_compute: SimpleLossCompute对象，用于计算损失\n",
    "    optimizer: Adam优化器。验证时，optimizer是DummyOptimizer\n",
    "    scheduler：LambdaLR对象，用于调整Adam的学习率，实现WarmUp\n",
    "               若对调整学习率不熟悉，可参考：https://blog.csdn.net/zhaohongfei_358/article/details/125759911\n",
    "               验证时，scheduler是DummyScheduler\n",
    "    accum_iter: 多少个batch更新一次参数，默认为1，也就是每个batch都对参数进行更新\n",
    "    train_state: TrainState对象，用于保存一些训练状态\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    # 记录target的总token数，每次打印日志后，进行清0\n",
    "    tokens = 0\n",
    "    # 记录tgt_y的总token数，用于对total_loss进行正则化\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    n_accum = 0 # 本次epoch更新了多少次模型参数\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # 前向传递。等价于model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
    "        # 但注意，这里的out是Decoder的输出，并不是Generator的输出，因为在EncoderDecoder\n",
    "        # 的forward中并没有使用generator。generator的调用放在了loss_compute中\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        计算损失，传入的三个参数分别为：\n",
    "        1. out: EncoderDecoder的输出，该值并没有过最后的线性层，过线性层被集成在了计算损失中\n",
    "        2. tgt_y: 要被预测的所有token，例如src为`<bos> I love you <eos>`，则`tgt_y`则为\n",
    "                  `我 爱 你 <eos>`\n",
    "        3. ntokens：这批batch中有效token的数量。用于对loss进行正则化。\n",
    "\n",
    "        返回两个loss，其中loss_node是正则化之后的，所以梯度下降时用这个。\n",
    "                    而loss是未进行正则化的，用于统计total_loss。\n",
    "        \"\"\"\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            # 计算梯度\n",
    "            loss_node.backward()\n",
    "            # 记录step次数\n",
    "            train_state.step += 1\n",
    "            # 记录样本数量。batch.src.shape[0]获取的是Batch size\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            # 记录处理过的token数\n",
    "            train_state.tokens += batch.ntokens\n",
    "\n",
    "            # 如果达到了accum_iter次，就进行一次参数更新\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                # 记录本次epoch的参数更新次数\n",
    "                n_accum += 1\n",
    "                # 记录模型的参数更新次数\n",
    "                train_state.accum_step += 1\n",
    "            # 更新学习率\n",
    "            scheduler.step()\n",
    "\n",
    "        # 累计loss\n",
    "        total_loss += loss\n",
    "        # 累计处理过的tokens\n",
    "        total_tokens += batch.ntokens\n",
    "        # 累计从上次打印日志开始处理过得tokens\n",
    "        tokens += batch.ntokens\n",
    "        # 每40个batch打印一次日志。\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            # 打印一下当前的学习率\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            # 记录这40个batch的消耗时间\n",
    "            elapsed = time.time() - start\n",
    "            # 打印日志\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                # i: 本次epoch的第几个batch\n",
    "                # n_accum: 本次epoch更新了多少次模型参数\n",
    "                # loss / batch.ntokens: 对loss进行正则化，然后再打印loss，其实这里可以直接用loss_node\n",
    "                # tokens / elapsed: 每秒可以处理的token数\n",
    "                # lr: 学习率（learning rate），这里打印学习率的目的是看一下warmup下学习率的变化\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            # 重置开始时间\n",
    "            start = time.time()\n",
    "            # 重置token数\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    # 返回正则化之后的total_loss，返回训练状态\n",
    "    return total_loss / total_tokens, train_state"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hrE36v4fVmRA"
   },
   "id": "hrE36v4fVmRA"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练数据和Batch\n",
    "\n",
    "我们在标准的WMT 2014 English-German数据集上进行了训练，该数据集包含450w个句子对儿。句子使用的是byte-pair编码（subword的一种方法，如果你对subword不了解，可以参考[这篇文章](https://blog.csdn.net/zhaohongfei_358/article/details/123379481)），其中source和target使用的是同一个词典。对于英文到法文的翻译，我们使用了一个巨大的WMT 2014 English-French数据集，其包含3600W个句子，其能拆分出的词典大小为32000。\n",
    "\n",
    "具有相似长度的句子对儿将会组成一个Batch。每个训练batch都会包含一组句子对儿，其包含大约25000个source tokens和25000个target tokens."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "M5IsXwP4VmRB"
   },
   "id": "M5IsXwP4VmRB"
  },
  {
   "cell_type": "markdown",
   "id": "3af07c4b",
   "metadata": {
    "id": "3af07c4b"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "Transformer使用了Warmup的方式来调整学习率，公式如下：\n",
    "\n",
    "$$\n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot\n",
    "  \\min({step\\_num}^{-0.5},\n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})\n",
    "$$\n",
    "\n",
    "等价于：\n",
    "\n",
    "$$\n",
    "lrate = \\frac{1}{\\sqrt{d_{model}}} min(\\frac{1}{\\sqrt{step\\_num}},  step\\_num \\cdot \\frac{1}{warmup\\_steps \\sqrt{warmup\\_steps}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "其中：\n",
    "\n",
    "- $lrate$: 学习率的调整参数。注意这个并不是学习率，假设你设置的学习率为0.8，则第$i$步的学习率为：$0.8 * lrate(i)$。\n",
    "- $d_{model}$：模型的维度，即词向量的维度\n",
    "- $step\\_num$: 步数。注意：执行一个backward（也可以说是一个batch）step加1，并不是optimizer.step()一次step加1.\n",
    "- warmup_steps: warmup多少步。在Transformer中使用的是4000，也就是预热4000步。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "YbvYaXZnVmRB"
   },
   "id": "YbvYaXZnVmRB"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ded57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:20.906007Z",
     "iopub.status.busy": "2022-05-02T01:25:20.904743Z",
     "iopub.status.idle": "2022-05-02T01:25:20.907701Z",
     "shell.execute_reply": "2022-05-02T01:25:20.907236Z"
    },
    "id": "27ded57a"
   },
   "outputs": [],
   "source": [
    "# 学习率调整函数\n",
    "def rate(step, model_size, factor, warmup):\n",
    "    # 避免分母为0\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    # 这里比原公式还多了一个factor，factor默认取1，相当于没有多。\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来是一个学习率调整的例子，如果你不熟悉自定义学习率，可以参考[这篇文章](https://blog.csdn.net/zhaohongfei_358/article/details/125759911)。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "8FsyxwjpVmRB"
   },
   "id": "8FsyxwjpVmRB"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043518a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:20.913235Z",
     "iopub.status.busy": "2022-05-02T01:25:20.912751Z",
     "iopub.status.idle": "2022-05-02T01:25:22.846903Z",
     "shell.execute_reply": "2022-05-02T01:25:22.846024Z"
    },
    "id": "043518a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def example_learning_schedule():\n",
    "    # 准备3个样例，三个参数分别为：d_model, factor, warmup_steps\n",
    "    opts = [\n",
    "        [512, 1, 4000],  # example 1\n",
    "        [512, 1, 8000],  # example 2\n",
    "        [256, 1, 4000],  # example 3\n",
    "    ]\n",
    "\n",
    "    # 随便定义一个没啥用的模型\n",
    "    dummy_model = torch.nn.Linear(1, 1)\n",
    "\n",
    "    # 记录这三个例子的学习率变化\n",
    "    learning_rates = []\n",
    "\n",
    "    # 分别运行上面的三个例子\n",
    "    for idx, example in enumerate(opts):\n",
    "        # 定义Adam优化器\n",
    "        optimizer = torch.optim.Adam(\n",
    "            dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n",
    "        )\n",
    "        # 定义LambdaLR调整学习率。\"*example\"称为序列解包\n",
    "        lr_scheduler = LambdaLR(\n",
    "            optimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\n",
    "        )\n",
    "        # 用于存储当前样例的学习率变化\n",
    "        tmp = []\n",
    "        # 进行20000次训练\n",
    "        for step in range(20000):\n",
    "            # 记录学习率\n",
    "            tmp.append(optimizer.param_groups[0][\"lr\"])\n",
    "            # 参数更新\n",
    "            optimizer.step()\n",
    "            # 更新学习率。更新学习率要放在optimizer.step()之后\n",
    "            lr_scheduler.step()\n",
    "        # 记录当前样例的学习率\n",
    "        learning_rates.append(tmp)\n",
    "\n",
    "    # 将学习率变化变成\n",
    "    learning_rates = torch.tensor(learning_rates)\n",
    "\n",
    "    # Enable altair to handle more than 5000 rows\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "\n",
    "    \"\"\"\n",
    "    组织统计图需要的数据，最终效果为：\n",
    "       |Learning Rate | model_size:warmup | step\n",
    "    ----------------------------------------------\n",
    "     0 |     1.74e-07 |          512:4000 |   0\n",
    "     1 |     1.74e-07 |          512:4000 |   1\n",
    "     2 |     3.49e-07 |          512:4000 |   2\n",
    "     ....\n",
    "    \"\"\"\n",
    "    opts_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Learning Rate\": learning_rates[warmup_idx, :],\n",
    "                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n",
    "                        warmup_idx\n",
    "                    ],\n",
    "                    \"step\": range(20000),\n",
    "                }\n",
    "            )\n",
    "            for warmup_idx in [0, 1, 2]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 绘制统计表\n",
    "    return (\n",
    "        alt.Chart(opts_data)\n",
    "        .mark_line()\n",
    "        .properties(width=600)\n",
    "        .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "example_learning_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e3c6e",
   "metadata": {
    "id": "aa2e3c6e"
   },
   "source": [
    "## 正则化\n",
    "\n",
    "### 标签平滑\n",
    "\n",
    "在Transformer的训练中，使用Label Smoothing技术对标签做了平滑处理，这样可以减少模型overconfidence，从而减少overfitting。\n",
    "\n",
    "公式为：\n",
    "\n",
    "$$\n",
    "P_{i} = \\begin{cases}\n",
    "   1 ~~~&\\text{if } (i=y) \\\\\n",
    "   0 ~~~&\\text{if } (i\\neq y))\n",
    "\\end{cases} ⇒ P_{i} = \\begin{cases}\n",
    "   1-\\epsilon ~~~&\\text{if } (i=y) \\\\\n",
    "   \\frac{\\epsilon}{K-1}  ~~~&\\text{if } (i\\neq y))\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中 $P_i$ 是标签的第i维的值； $K$为类别总数；$\\epsilon$为平滑因子，取值范围为$[0,1]$, 通常为0.1，\n",
    "\n",
    "例如，假设我们的标签是2, 词典大小为5，则对应的one-hot向量则为：$[0, 0, 0, 1, 0]$，我们现在取平滑因子$\\epsilon=0.2$，则平滑后的Label为：\n",
    "\n",
    "$$[0.2/4, 0.2/4, 0.2/4, 1-0.2, 0.2/4]=[0.05, 0.05, 0.05, 0.8, 0.05]$$\n",
    "\n",
    "之后计算loss时，使用平滑后的标签。直观上的理解就是：模型你别太自信，就算你预测对了也对你进行一点惩罚，防止你过度相信这个特定样本的特定结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "不过在下面的LabelSmoothing的实现与上面说的略有不同，主要有两点：\n",
    "\n",
    "1. 这个LabelSmoothing类将损失计算也放了进来，也就是它除了负责标签平滑外，还负责计算损失。\n",
    "2. 由于词典中有一项是“填充”(`<pad>`)，而网络在什么情况下都不应该预测为`<pad>`，所以在LabelSmoothing的时候不应该让`<pad>`参与。\n",
    "\n",
    "> 在本代码实现中，所有的`<blank>`指都是`<pad>`\n",
    "\n",
    "对于第二点，做个详细的说明。\n",
    "\n",
    "假设我们的词典为`{0: <pad>, 1: <bos>, 2: <eos>, 3: I, 4: love, ..., 1999: you}`，此时词典大小为2000，所以one-hot向量的维度也是2000。如果我们做预测，无论什么情况我们的标签都不应该是`<pad>`（[1, 0, 0, ...]），因为我们不预测`<pad>`。所以我们在平滑的时候，应该忽略`<blank>`，所以公式中的 $K-1$ 也要变成 $K-2$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "_INM8BNrVmRB"
   },
   "id": "_INM8BNrVmRB"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d96f50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:22.858827Z",
     "iopub.status.busy": "2022-05-02T01:25:22.858459Z",
     "iopub.status.idle": "2022-05-02T01:25:22.860907Z",
     "shell.execute_reply": "2022-05-02T01:25:22.861184Z"
    },
    "id": "77d96f50",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    实现标签平滑。\n",
    "    该类除了标签平滑外，还包含了损失的计算。\n",
    "    所以此类为 LabelSmoothing + LossFunction\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        \"\"\"\n",
    "        size: 目标词典的大小。\n",
    "        padding_idx: 空格('<blank>')在词典中对应的index，`<blank>`等价于`<pad>`\n",
    "        smoothing: 平滑因子，0表示不做平滑处理\n",
    "        \"\"\"\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        # 定义损失函数，这里使用的是KL Divergence Loss，也是一种多分类常用的损失函数\n",
    "        # KLDivLoss官方文档：https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        # true distribution，平滑后的标签\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        x: generator输出的概率分布。Shape为(batch_size, 词典大小)\n",
    "        target: 目标标签。Shape为(batch_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # 确保generator的输出维度和词典大小一致，否则后面计算loss的时候就会出错\n",
    "        assert x.size(1) == self.size\n",
    "\n",
    "        # 这句其实是为了创建一个与x有相同Shape的tensor。\n",
    "        # 这里假设x的shape为(2, 6)，即batch_size为2，词典大小为6。\n",
    "        true_dist = x.data.clone()\n",
    "\n",
    "        \"\"\"\n",
    "        将true_dist全部填充为 self.smoothing / (self.size - 2)。\n",
    "        假设 smoothing=0.2，则为全部填充为 0.2 / 4= 0.05\n",
    "        此时true_dist则为：\n",
    "        [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
    "         [0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]\n",
    "        \"\"\"\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "\n",
    "        \"\"\"\n",
    "        scatter_: 官方地址 https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html\n",
    "                  将true_dist的1维度上与target.data.unsqueeze(1)对应的值变为src。\n",
    "        假设此例中target.data.unsqueeze(1) 为[[2], [3]]，即2个数据的标签分别为2，3\n",
    "        则true_dist执行过scatter后变为:\n",
    "        [[0.05, 0.05, 0.8, 0.05, 0.05, 0.05],\n",
    "         [0.05, 0.05, 0.05, 0.8, 0.05, 0.05]]\n",
    "        \"\"\"\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        # 将\"空格\"所在的index填充为0。\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        # 找出target中，label为<blank>的标签。例如target为['i', 'love', 'you', '<blank>', '<blank>']\n",
    "        # 那么mask则为[[1], [3]]，表示第1个和第3个为空格。\n",
    "        # 但在实际应用场景中标签中不应该出现\"<blank>\"\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            \"\"\"\n",
    "            将\"<blank>\"所在的label整个设置为0。\n",
    "            例如，假设现在true_dist为：\n",
    "            [[0.05, 0.05, 0.8, 0.05, 0.05, 0.05],\n",
    "             [0.05, 0.05, 0.05, 0.8, 0.05, 0.05],\n",
    "             [0.8, 0.05, 0.05, 0.05, 0.05, 0.05]]\n",
    "            其中第3行为“<blank>”的label，则执行完这行代码后，则为：\n",
    "            [[0.05, 0.05, 0.8, 0.05, 0.05, 0.05],\n",
    "             [0.05, 0.05, 0.05, 0.8, 0.05, 0.05],\n",
    "             [0.00, 0.00, 0.00, 0.0, 0.00, 0.00]]\n",
    "            \"\"\"\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        # 保存平滑标签后的label\n",
    "        self.true_dist = true_dist\n",
    "\n",
    "        \"\"\"\n",
    "        使用平滑后的标签计算损失\n",
    "        由于对`<blank>`部分进行了mask，所以在这部分是不会参与损失计算的\n",
    "        \"\"\"\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "A24-Q_4eVmRC"
   },
   "id": "A24-Q_4eVmRC"
  },
  {
   "cell_type": "markdown",
   "id": "cd102b03",
   "metadata": {
    "id": "cd102b03"
   },
   "source": [
    "这里对来看一个Label smoothing的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def example_label_smoothing():\n",
    "    # 定义LabelSmoothing, 词典大小为5， <blank>对应index为0，平滑因子为0.4\n",
    "    crit = LabelSmoothing(5, 0, 0.4)\n",
    "    # 定义predict，即网络的预测分布。\n",
    "    # 这里的1e-9在源码中为0。由于KLDivLoss需要对predict求log，而0会导致结果为负无穷\n",
    "    # 所以我这里将0改成了1e-9。\n",
    "    predict = torch.FloatTensor(\n",
    "        [\n",
    "            [1e-9, 0.2, 0.7, 0.1, 1e-9],\n",
    "            [1e-9, 0.2, 0.7, 0.1, 1e-9],\n",
    "            [1e-9, 0.2, 0.7, 0.1, 1e-9],\n",
    "            [1e-9, 0.2, 0.7, 0.1, 1e-9],\n",
    "            [1e-9, 0.2, 0.7, 0.1, 1e-9],\n",
    "        ]\n",
    "    )\n",
    "    loss = crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\n",
    "\n",
    "    print(\"loss:\", loss)\n",
    "    print(\"Before label smoothing:\\n\", torch.zeros(5, 6).scatter_(1, torch.LongTensor([2, 1, 0, 3, 3]).unsqueeze(1), 1))\n",
    "    print(\"After label smoothing:\\n\", crit.true_dist)\n",
    "\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"target distribution\": crit.true_dist[x, y].flatten(),\n",
    "                    \"columns\": y,\n",
    "                    \"rows\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(5)\n",
    "            for x in range(5)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect(color=\"Blue\", opacity=1)\n",
    "        .properties(height=200, width=200)\n",
    "        .encode(\n",
    "            alt.X(\"columns:O\", title=None),\n",
    "            alt.Y(\"rows:O\", title=None),\n",
    "            alt.Color(\n",
    "                \"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n",
    "            ),\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "example_label_smoothing()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ggOYgVEIVmRC"
   },
   "id": "ggOYgVEIVmRC"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 上图中越黄，表示这个位置的值越接近1, 反之则越接近0。没平滑前，应该是黄色的部分巨黄（=1），其他部位都是巨紫（=0），平滑之后，黄色部位的黄色素给其他紫的部分匀了一些（除了'\\<blank\\>'位置）。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "spTR_PJpVmRC"
   },
   "id": "spTR_PJpVmRC"
  },
  {
   "cell_type": "markdown",
   "id": "dd4ba439",
   "metadata": {
    "id": "dd4ba439"
   },
   "source": [
    "下面是另一个平滑的例子，从该例子可以看到当模型非常自信的时候就会给予其一个微小的惩罚："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss(x, crit):\n",
    "    # x是从0到100的一个不断增大的数。 d=x+3，比x大一点。\n",
    "    d = x + 3\n",
    "    \"\"\"\n",
    "    模拟模型的输出。\n",
    "    一开始x为1，模型输出为：[[0.0000, 0.2500, 0.2500, 0.2500, 0.2500]]\n",
    "              此时模型模型还不太会预测\n",
    "    当x到100时，模型输出为：[[0.0000, 0.9706, 0.0098, 0.0098, 0.0098]]\n",
    "              此时模型可以很自信的说结果就是 1\n",
    "    \"\"\"\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    # 计算模型损失。由于使用的是KLDivLoss，所以要对predict进行log操作\n",
    "    return crit(predict.log(), torch.LongTensor([1])).data\n",
    "\n",
    "\n",
    "def penalization_visualization():\n",
    "    crit = LabelSmoothing(5, 0, 0.1)\n",
    "    loss_data = pd.DataFrame(\n",
    "        {\n",
    "            # x从1开始不断增大，模拟模型的表现越来越好\n",
    "            \"Loss\": [loss(x, crit) for x in range(1, 100)],\n",
    "            \"Steps\": list(range(99)),\n",
    "        }\n",
    "    ).astype(\"float\")\n",
    "\n",
    "    return (\n",
    "        alt.Chart(loss_data)\n",
    "        .mark_line()\n",
    "        .properties(width=350)\n",
    "        .encode(\n",
    "            x=\"Steps\",\n",
    "            y=\"Loss\",\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "penalization_visualization()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_fe8pFZhVmRC"
   },
   "id": "_fe8pFZhVmRC"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 从上图可以看出，大约到20次的时候，惩罚效果就出来了。模型虽然很自信的能说出正确答案，但是给他一个小的惩罚，越自信，损失反而越大。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "B-AnlndKVmRC"
   },
   "id": "B-AnlndKVmRC"
  },
  {
   "cell_type": "markdown",
   "id": "9eb37970",
   "metadata": {
    "id": "9eb37970"
   },
   "source": [
    "# 第一个例子\n",
    "\n",
    "我们先从一个最简单的例子开始：从一个很小的词典中随机选取一些词做为输入，目标就是重新输出这些词。我们称为**copy任务**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee6655",
   "metadata": {
    "id": "18ee6655"
   },
   "source": [
    "## 造数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334bdb3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.456326Z",
     "iopub.status.busy": "2022-05-02T01:25:23.455411Z",
     "iopub.status.idle": "2022-05-02T01:25:23.457254Z",
     "shell.execute_reply": "2022-05-02T01:25:23.457943Z"
    },
    "id": "334bdb3f"
   },
   "outputs": [],
   "source": [
    "def data_gen(V, batch_size, nbatches):\n",
    "    \"\"\"\n",
    "    生成一组随机数据。（该方法仅用于Demo）\n",
    "    :param V: 词典的大小\n",
    "    :param batch_size\n",
    "    :param nbatches: 生成多少个batch\n",
    "    :return: yield一个Batch对象\n",
    "    \"\"\"\n",
    "\n",
    "    # 生成{nbatches}个batch\n",
    "    for i in range(nbatches):\n",
    "        # 生成一组输入数据\n",
    "        data = torch.randint(1, V, size=(batch_size, 10))\n",
    "        # 将每行的第一个词都改为1，即\"<bos>\"\n",
    "        data[:, 0] = 1\n",
    "        # 该数据不需要梯度下降\n",
    "        src = data.requires_grad_(False).clone().detach()\n",
    "        tgt = data.requires_grad_(False).clone().detach()\n",
    "        # 返回一个Batch对象\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe4433",
   "metadata": {
    "id": "33fe4433"
   },
   "source": [
    "## 损失计算"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面的类可不只是例子要用，最后的实战部分也要用："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "8UmrzCdnVmRD"
   },
   "id": "8UmrzCdnVmRD"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88cbe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.462842Z",
     "iopub.status.busy": "2022-05-02T01:25:23.461999Z",
     "iopub.status.idle": "2022-05-02T01:25:23.464005Z",
     "shell.execute_reply": "2022-05-02T01:25:23.464702Z"
    },
    "id": "4a88cbe5"
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"\n",
    "    一个简单的损失计算和训练函数。\n",
    "    该类除了包含损失计算外，还包含模型generator部分的前向传递。\n",
    "    如果你对上面这句话不太理解，可参考这篇文章：\n",
    "    https://blog.csdn.net/zhaohongfei_358/article/details/125759911\n",
    "    请参考章节：Pytorch 实现梯度下降与参数更新\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        \"\"\"\n",
    "        generator: Generator类对象，用于根据Decoder的输出预测下一个token\n",
    "        criterion: LabelSmoothing类对象，用于对Label进行平滑和计算损失\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        \"\"\"\n",
    "        x: EncoderDecoder的输出，也就是Decoder的输出\n",
    "        y: batch.tgt_y，要被预测的所有token，例如src为`<bos> I love you <eos>`，\n",
    "           则`tgt_y`则为`我 爱 你 <eos>`\n",
    "        norm: batch.ntokens, tgt_y中的有效token数。用于对loss进行正则化。\n",
    "        \"\"\"\n",
    "\n",
    "        # 调用generator预测token。（EncoderDecoder的forward中并没有调用generator）\n",
    "        x = self.generator(x)\n",
    "        \"\"\"\n",
    "        这里首先使用KLDivLoss进行了损失计算，随后又除以batch.ntokens对loss进行正则化。\n",
    "        \"\"\"\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里详细说明一下上面的正则化，直接通过例子吧。\n",
    "\n",
    "假设我们的第一个batch（假设batch_size=2）的tgt_y对应的句子为：`<bos> 我 爱 你 <eos> <pad> <pad> ...` 和 `<bos> 苹果 好 吃 <eos> <pad> <pad>`。\n",
    "\n",
    "则我们的loss相当于对8个预测结果进行了计算，即`我 爱 你 <eos> 苹果 好 吃 <eos>`，也就是$loss = \\sum_{i=1}^{8}L(token_i)$\n",
    "\n",
    "假设我们的第二个batch的tgt_y对应的句子为：`<bos> 中国 是 ...(此处省略100个字) <eos>` 和 `<bos> 华夏 文明 发源 .. <eos>`。\n",
    "\n",
    "则我们的loss岘港与对200个预测结果进行了计算，也就是$loss = \\sum_{i=1}^{200}L(token_i)$\n",
    "\n",
    "如果不做平均的话，显然第二个大，然后就会出现loss一会大一会小的情况，所以要求一下平均，也就是除以有效的token数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "pXO6m6_7VmRD"
   },
   "id": "pXO6m6_7VmRD"
  },
  {
   "cell_type": "markdown",
   "id": "00b7bdc2",
   "metadata": {
    "id": "00b7bdc2"
   },
   "source": [
    "## 使用贪心算法解码(Greedy Decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958119b5",
   "metadata": {
    "id": "958119b5",
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "简单起见，使用贪心算法解码(Greedy Decoding)进行翻译任务的预测。\n",
    "\n",
    "在这里，所谓的贪心算法就是Transformer的正常推理过程：先求出encoder的输出memory，然后利用memory一个一个的求出token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426fe69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.470843Z",
     "iopub.status.busy": "2022-05-02T01:25:23.469910Z",
     "iopub.status.idle": "2022-05-02T01:25:23.471795Z",
     "shell.execute_reply": "2022-05-02T01:25:23.472524Z"
    },
    "id": "8426fe69",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 这个代码其实和最开始的inference_test()是一样的\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    进行模型推理，推理出所有预测结果。\n",
    "    :param model: Transformer模型，即EncoderDecoder类对象\n",
    "    :param src: Encoder的输入inputs，Shape为(batch_size, 词数)\n",
    "                例如：[[1, 2, 3, 4, 5, 6, 7, 8, 0, 0]]\n",
    "                即一个句子，该句子有10个词，分别为1,2,...,0\n",
    "    :param src_mask: src的掩码，掩盖住非句子成分。\n",
    "    :param max_len: 一个句子的最大长度。\n",
    "    :param start_symbol: '<bos>' 对应的index，在本例中始终为0\n",
    "    :return: 预测结果，例如[[1, 2, 3, 4, 5, 6, 7, 8]]\n",
    "    \"\"\"\n",
    "\n",
    "    # 将src送入Transformer的Encoder，输出memory\n",
    "    memory = model.encode(src, src_mask)\n",
    "    # 初始化ys为[[0]]，用于保存预测结果，其中0表示'<bos>'\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # 循环调用decoder，一个个的进行预测。例如：假设我们要将“I love you”翻译成\n",
    "    # “我爱你”，则第一次的`ys`为(<bos>)，然后输出为“I”。然后第二次`ys`为(<bos>, I)\n",
    "    # 输出为\"love\"，依次类推，直到decoder输出“<eos>”或达到句子最大长度。\n",
    "    for i in range(max_len - 1):\n",
    "        # 将encoder的输出memory和之前Decoder的所有输出作为参数，让Decoder来预测下一个token\n",
    "        out = model.decode(\n",
    "            # ys就是Decoder之前的所有输出\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        # 将Decoder的输出送给generator进行预测。这里只取最后一个词的输出进行预测。\n",
    "        # 因为你传的tgt的词数是变化的，第一次是(<bos>)，第二次是(<bos>, I)\n",
    "        # 所以你的out的维度也是变化的，变化的就是(batch_size, 词数，词向量)中词数这个维度\n",
    "        # 前面的词向量送给generator预测的话，预测出来的也是前面的词，所以只能取最后一个。\n",
    "        prob = model.generator(out[:, -1])\n",
    "        # 取出数值最大的那个，它的index在词典中对应的词就是预测结果\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        # 取出预测结果\n",
    "        next_word = next_word.data[0]\n",
    "        # 将这一次的预测结果和之前的拼到一块，作为之后Decoder的输入\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "\n",
    "    # 返回最终的预测结果。\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来进行一个简单的模型训练，来实现copy任务。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "j8ydJ_EFVmRD"
   },
   "id": "j8ydJ_EFVmRD"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a93be76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.481831Z",
     "iopub.status.busy": "2022-05-02T01:25:23.480882Z",
     "iopub.status.idle": "2022-05-02T01:25:23.482798Z",
     "shell.execute_reply": "2022-05-02T01:25:23.483742Z"
    },
    "id": "6a93be76",
    "lines_to_next_cell": 2,
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1659239391541,
     "user_tz": -480,
     "elapsed": 239932,
     "user": {
      "displayName": "Garen Snail",
      "userId": "12776734623535172445"
     }
    },
    "outputId": "8dbe1bf6-4162-4e2d-c16f-6190c3772329"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   3.15 | Tokens / Sec:   822.1 | Learning Rate: 5.5e-06\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   2.05 | Tokens / Sec:   862.9 | Learning Rate: 6.1e-05\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.75 | Tokens / Sec:   988.5 | Learning Rate: 1.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.43 | Tokens / Sec:   964.5 | Learning Rate: 1.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.04 | Tokens / Sec:   975.5 | Learning Rate: 2.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.60 | Tokens / Sec:   973.4 | Learning Rate: 2.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.36 | Tokens / Sec:   991.6 | Learning Rate: 3.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.13 | Tokens / Sec:   979.4 | Learning Rate: 3.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.14 | Tokens / Sec:   990.4 | Learning Rate: 4.5e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.11 | Tokens / Sec:   973.2 | Learning Rate: 5.0e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.19 | Tokens / Sec:   989.5 | Learning Rate: 5.6e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.12 | Tokens / Sec:   986.4 | Learning Rate: 6.1e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.20 | Tokens / Sec:   986.2 | Learning Rate: 6.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:   992.6 | Learning Rate: 7.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.13 | Tokens / Sec:   996.2 | Learning Rate: 7.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.07 | Tokens / Sec:   982.8 | Learning Rate: 8.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.12 | Tokens / Sec:  1000.1 | Learning Rate: 8.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.19 | Tokens / Sec:   986.6 | Learning Rate: 9.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.12 | Tokens / Sec:   996.8 | Learning Rate: 1.0e-03\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.08 | Tokens / Sec:   999.6 | Learning Rate: 1.1e-03\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# 训练一个简单的copy任务\n",
    "def example_simple_model():\n",
    "    # 定义词典大小为11\n",
    "    V = 11\n",
    "\n",
    "    # 定义损失函数\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "\n",
    "    # 构建模型，src和tgt的词典大小都为2，Layer数量为2\n",
    "    model = make_model(V, V, N=2)\n",
    "\n",
    "    # 使用Adam优化器\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "\n",
    "    # 自定义Warmup学习率\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    batch_size = 80\n",
    "    # 运行20个epoch\n",
    "    for epoch in range(20):\n",
    "        # 将模型调整为训练模式\n",
    "        model.train()\n",
    "        # 训练一个Batch\n",
    "        run_epoch(\n",
    "            # 生成20个batch对象\n",
    "            data_gen(V, batch_size, 20),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train\",\n",
    "        )\n",
    "        model.eval()\n",
    "        # 在一个epoch后，进行模型验证\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 5),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            # 不进行参数更新\n",
    "            DummyOptimizer(),\n",
    "            # 不调整学习率\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )[0]  # run_epoch返回loss和train_state，这里取loss，所以是[0]\n",
    "        # 但是源码中并没有接收这个loss，所以这个[0]实际上没什么意义\n",
    "\n",
    "    # 将模型调整为测试模式，准备开始推理\n",
    "    model.eval()\n",
    "    # 定义一个src 0-9，看看model能不能重新输出0-9\n",
    "    src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "    # 句子最大长度就是10,\n",
    "    max_len = src.shape[1]\n",
    "    # 不需要掩码，因为这10个都是有意义的数字\n",
    "    src_mask = torch.ones(1, 1, max_len)\n",
    "    # 使用greedy_decoder函数进行推理\n",
    "    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))\n",
    "\n",
    "\n",
    "example_simple_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "如果运行了这个例子，你可能会有一点小疑问：①为什么Epoch Step始终为1；②为什么Accumulation Step始终为2\n",
    "\n",
    "1. 关于第一点：首先，Epoch Step指的是**在本次epoch中**执行到了第几个batch(从0开始)。其次因为在run_epoch中，会在第1,41,81,...个batch打印日志，而在本例中一共就20个batch，所以一个epoch只会打印一次日志。\n",
    "2. 第二点：Accumulation Step指的是**在本次epoch中**执行参数更新(`optimizier.step()`)的次数。在第一打印日志时，一共执行了2次参数更新，又因为一个epoch只会打印一次日志，所以该值始终为2。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "KMe1I7E1VmRE"
   },
   "id": "KMe1I7E1VmRE"
  },
  {
   "cell_type": "markdown",
   "id": "9a439ef0",
   "metadata": {
    "id": "9a439ef0"
   },
   "source": [
    "# Part 3: 实战：德译英\n",
    "\n",
    "现在我们来进行一个案例实战，我们使用Multi30k German-English 翻译任务。虽然这个任务远小于论文中的WMT任务，但也足以阐明整个系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb7001",
   "metadata": {
    "id": "1ceb7001",
    "tags": []
   },
   "source": [
    "## 数据加载\n",
    "\n",
    "我们将使用`torchtext`进行数据加载，并使用spacy进行分词。spacy可以参考[这篇文章](https://blog.csdn.net/zhaohongfei_358/article/details/125469155)。\n",
    "\n",
    "> 加载数据集一定要使用这两个版本`torchdata==0.3.0, torchtext==0.12`，否则会加载失败。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "加载分词模型，如果你还没有下载，请使用如下代码进行下载（代码中也会有）：\n",
    "\n",
    "```\n",
    "python -m spacy download de_core_news_sm\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "若在国内使用命令下载失败，请使用离线下载。（注意版本需是3.2.0）[de_core_news_sm下载链接](https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-3.2.0)，[en_core_web_sm下载链接](https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.2.0)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "XK_Q5hq2VmRE"
   },
   "id": "XK_Q5hq2VmRE"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c3a83e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.491667Z",
     "iopub.status.busy": "2022-05-02T01:25:23.490728Z",
     "iopub.status.idle": "2022-05-02T01:25:23.492689Z",
     "shell.execute_reply": "2022-05-02T01:25:23.493407Z"
    },
    "id": "9c3a83e3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1659239391542,
     "user_tz": -480,
     "elapsed": 2,
     "user": {
      "displayName": "Garen Snail",
      "userId": "12776734623535172445"
     }
    }
   },
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "    \"\"\"\n",
    "    加载spacy分词模型\n",
    "    :return: 返回德语分词模型和英语分词模型\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except IOError:\n",
    "        # 如果报错，说明还未安装分词模型，进行安装后重新加载\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # 返回德语分词模型和英语分词模型\n",
    "    return spacy_de, spacy_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6092be4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.500603Z",
     "iopub.status.busy": "2022-05-02T01:25:23.499698Z",
     "iopub.status.idle": "2022-05-02T01:25:23.502449Z",
     "shell.execute_reply": "2022-05-02T01:25:23.501724Z"
    },
    "id": "6092be4f",
    "lines_to_next_cell": 2,
    "tags": [],
    "executionInfo": {
     "status": "ok",
     "timestamp": 1659239391542,
     "user_tz": -480,
     "elapsed": 2,
     "user": {
      "displayName": "Garen Snail",
      "userId": "12776734623535172445"
     }
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer):\n",
    "    \"\"\"\n",
    "    对text文本进行分词\n",
    "    :param text: 要分词的文本，例如“I love you”\n",
    "    :param tokenizer: 分词模型，例如：spacy_en\n",
    "    :return: 分词结果，例如 [\"I\", \"love\", \"you\"]\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    \"\"\"\n",
    "    yield一个token list\n",
    "    :param data_iter: 包含句子对儿的可迭代对象。例如：\n",
    "                      [(\"I love you\", \"我爱你\"), ...]\n",
    "    :param tokenizer: 分词模型。例如spacy_en\n",
    "    :param index: 要对句子对儿的哪个语言进行分词，\n",
    "                  例如0表示对上例的英文进行分词\n",
    "    :return: yield本轮的分词结果，例如['I', 'love', 'you']\n",
    "    \"\"\"\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield tokenizer(from_to_tuple[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64878fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:23.516475Z",
     "iopub.status.busy": "2022-05-02T01:25:23.515578Z",
     "iopub.status.idle": "2022-05-02T01:25:25.130885Z",
     "shell.execute_reply": "2022-05-02T01:25:25.130120Z"
    },
    "id": "64878fb2",
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1659239410945,
     "user_tz": -480,
     "elapsed": 19405,
     "user": {
      "displayName": "Garen Snail",
      "userId": "12776734623535172445"
     }
    },
    "outputId": "d4c9a701-da4a-47de-f5a8-48339c733839"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building German Vocabulary ...\n",
      "Building English Vocabulary ...\n",
      "Finished.\n",
      "Vocabulary sizes:\n",
      "vocab_src size: 8315\n",
      "vocab_tgt size: 6384\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(spacy_de, spacy_en):\n",
    "    \"\"\"\n",
    "    构建德语词典和英语词典\n",
    "    :return: 返回德语词典和英语词典，均为：Vocab对象\n",
    "             Vocab对象官方地址为：https://pytorch.org/text/stable/vocab.html#vocab\n",
    "    \"\"\"\n",
    "    # 构建德语分词方法\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    # 构建英语分词方法\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    print(\"Building German Vocabulary ...\")\n",
    "\n",
    "    \"\"\"\n",
    "    其中train, val, test都是可迭代对象。\n",
    "    例如：next(iter(train)) 返回一个tuple，为：\n",
    "    ('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
    "     'Two young, White males are outside near many bushes.')\n",
    "    \"\"\"\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "\n",
    "    \"\"\"\n",
    "    build_vocab_from_iterator：根据一个可迭代对象生成一个词典。\n",
    "    其返回一个Vocab对象，官方地址为：https://pytorch.org/text/stable/vocab.html#vocab\n",
    "\n",
    "    其接收是三个参数\n",
    "    1. iterator，需要传入一个可迭代对象。里面为分好词的数据，例如：\n",
    "                [(\"I\", \"love\", \"you\"), (\"you\", \"love\", \"me\")]\n",
    "    2. min_freq，最小频率，当一个单词的出现频率达到最小频率后才会被\n",
    "                 算到词典中。例如，如果min_freq=2，则上例中只有“you”\n",
    "                 会被算到词典中，因为其他单词都只出现一次。\n",
    "    3.specials， 特殊词汇，例如'<bos>', '<unk>'等。特殊单词会被加到\n",
    "                 词典的最前面。\n",
    "\n",
    "    假设我们调用的是:\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        [(\"I\", \"love\", \"you\"), (\"you\", \"love\", \"me\")],\n",
    "        min_freq=1,\n",
    "        specials=[\"<s>\", \"</s>\"],\n",
    "    )\n",
    "    vocab对应的词典则为：{0:<s>, 1:</s>, 2:love, 3:you, 4:I, 5:me}\n",
    "    \"\"\"\n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_de, index=0),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    # 开始构建英语词典，与上面一样\n",
    "    print(\"Building English Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_en, index=1),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    # 设置默认index为`<unk>`，后面对于那些不认识的单词就会自动归为`<unk>`\n",
    "    vocab_src.set_default_index(vocab_src[\"<unk>\"])\n",
    "    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n",
    "\n",
    "    # 返回构建好的德语词典和英语词典\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "def load_vocab(spacy_de, spacy_en):\n",
    "    \"\"\"\n",
    "    加载德语词典和英语词典。由于构建词典的过程需要花费一定时间，\n",
    "    所以该方法就是对build_vocabulary的进一步封装，增加了缓存机制。\n",
    "    :return: 返回德语词典和英语词典，均为Vocab对象\n",
    "    \"\"\"\n",
    "\n",
    "    # 如果不存在缓存文件，说明是第一次构建词典\n",
    "    if not exists(\"vocab.pt\"):\n",
    "        # 构建词典，并写入缓存文件\n",
    "        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n",
    "        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n",
    "    else:\n",
    "        # 如果存在缓存文件，直接加载\n",
    "        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    # 输出些日志：\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(\"vocab_src size:\", len(vocab_src))\n",
    "    print(\"vocab_tgt size:\", len(vocab_tgt))\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "# 全局参数，后续还要用\n",
    "# 加载德语和英语分词器\n",
    "spacy_de, spacy_en = load_tokenizers()\n",
    "# 加载德语词典（源词典）和英语词典（目标词典）\n",
    "vocab_src, vocab_tgt = load_vocab(spacy_de, spacy_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57752f8",
   "metadata": {
    "incorrectly_encoded_metadata": "id=\"kDEj-hCgokC-\" tags=[] jp-MarkdownHeadingCollapsed=true",
    "id": "f57752f8"
   },
   "source": [
    "## Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33350048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:25.144310Z",
     "iopub.status.busy": "2022-05-02T01:25:25.143931Z",
     "iopub.status.idle": "2022-05-02T01:25:25.146354Z",
     "shell.execute_reply": "2022-05-02T01:25:25.146629Z"
    },
    "id": "33350048",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_batch(\n",
    "    batch,\n",
    "    src_pipeline,\n",
    "    tgt_pipeline,\n",
    "    src_vocab,\n",
    "    tgt_vocab,\n",
    "    device,\n",
    "    max_padding=128,\n",
    "    pad_id=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Dataloader中的collate_fn函数。该函数的作用是：将文本句子处理成数字句子，然后pad到固定长度，最终batch到一起\n",
    "\n",
    "    :param batch: 一个batch的语句对。例如：\n",
    "                  [('Ein Kleinkind ...', 'A toddler in ...'), # [(德语), (英语)\n",
    "                   ....                                       # ...\n",
    "                   ...]                                       # ... ]\n",
    "    :param src_pipeline: 德语分词器，也就是tokenize_de方法，后面会定义\n",
    "                         其实就是对spacy_de的封装\n",
    "    :param tgt_pipeline: 英语分词器，也就是tokenize_en方法\n",
    "    :param src_vocab: 德语词典，Vocab对象\n",
    "    :param tgt_vocab: 英语词典，Vocab对象\n",
    "    :param device: cpu或cuda\n",
    "    :param max_padding: 句子的长度。pad长度不足的句子和裁剪长度过长的句子，\n",
    "                        目的是让不同长度的句子可以组成一个tensor\n",
    "    :param pad_id: '<blank>'在词典中对应的index\n",
    "    :return: src和tgt。处理后并batch后的句子。例如：\n",
    "             src为：[[0, 4354, 314, ..., 1, 2, 2, ..., 2],  [0, 4905, 8567, ..., 1, 2, 2, ..., 2]]\n",
    "             其中0是<bos>, 1是<eos>, 2是<blank>\n",
    "             src的Shape为(batch_size, max_padding)\n",
    "             tgt同理。\n",
    "    \"\"\"\n",
    "\n",
    "    # 定义'<bos>'的index，在词典中为0，所以这里也是0\n",
    "    bs_id = torch.tensor([0], device=device)  # <s> token id\n",
    "    # 定义'<eos>'的index\n",
    "    eos_id = torch.tensor([1], device=device)  # </s> token id\n",
    "\n",
    "    # 用于存储处理后的src和tgt\n",
    "    src_list, tgt_list = [], []\n",
    "    # 循环遍历句子对儿\n",
    "    for (_src, _tgt) in batch:\n",
    "        \"\"\"\n",
    "        _src: 德语句子，例如：Ein Junge wirft Blätter in die Luft.\n",
    "        _tgt: 英语句子，例如：A boy throws leaves into the air.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        将句子进行分词，并将词转成对应的index。例如：\n",
    "        \"I love you\" -> [\"I\", \"love\", \"you\"] ->\n",
    "        [1136, 2468, 1349] -> [0, 1136, 2468, 1349, 1]\n",
    "        其中0,1是<bos>和<eos>。\n",
    "\n",
    "        Vocab对象可以将list中的词转为index，例如：\n",
    "        `vocab_tgt([\"I\", \"love\", \"you\"])` 的输出为：\n",
    "        [1136, 2468, 1349]\n",
    "        \"\"\"\n",
    "        processed_src = torch.cat(\n",
    "            # 将<bos>，句子index和<eos>拼到一块\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    # 进行分词后，转换为index。\n",
    "                    src_vocab(src_pipeline(_src)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        processed_tgt = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    tgt_vocab(tgt_pipeline(_tgt)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        将长度不足的句子进行填充到max_padding的长度的，然后增添到list中\n",
    "\n",
    "        pad：假设processed_src为[0, 1136, 2468, 1349, 1]\n",
    "             第二个参数为: (0, 72-5)\n",
    "             第三个参数为：2\n",
    "        则pad的意思表示，给processed_src左边填充0个2，右边填充67个2。\n",
    "        最终结果为：[0, 1136, 2468, 1349, 1, 2, 2, 2, ..., 2]\n",
    "        \"\"\"\n",
    "        src_list.append(\n",
    "            pad(\n",
    "                processed_src,\n",
    "                (0, max_padding - len(processed_src),),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "        tgt_list.append(\n",
    "            pad(\n",
    "                processed_tgt,\n",
    "                (0, max_padding - len(processed_tgt),),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 将多个src句子堆叠到一起\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "\n",
    "    # 返回batch后的结果\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d94df2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:25.158739Z",
     "iopub.status.busy": "2022-05-02T01:25:25.158133Z",
     "iopub.status.idle": "2022-05-02T01:25:25.160905Z",
     "shell.execute_reply": "2022-05-02T01:25:25.160371Z"
    },
    "id": "3d94df2f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    device,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    batch_size=12000,\n",
    "    max_padding=128\n",
    "):\n",
    "    \"\"\"\n",
    "    创建train_dataloader和valid_dataloader\n",
    "    :param device: cpu或cuda\n",
    "    :param vocab_src: 源词典，本例中为德语词典\n",
    "    :param vocab_tgt: 目标词典，本例中为英语词典\n",
    "    :param spacy_de: 德语分词器\n",
    "    :param spacy_en: 英语分词器\n",
    "    :param batch_size: batch_size\n",
    "    :param max_padding: 句子的最大长度\n",
    "\n",
    "    :return: train_dataloader和valid_dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    # 定义德语分词器\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    # 定义英语分词器\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    # 创建批处理工具，即应该如何将一批数据汇总成一个Batch\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(\n",
    "            batch,\n",
    "            tokenize_de,\n",
    "            tokenize_en,\n",
    "            vocab_src,\n",
    "            vocab_tgt,\n",
    "            device,\n",
    "            max_padding=max_padding,\n",
    "            pad_id=vocab_src.get_stoi()[\"<blank>\"],\n",
    "        )\n",
    "\n",
    "    # 加载数据集\n",
    "    train_iter, valid_iter, test_iter = datasets.Multi30k(\n",
    "        language_pair=(\"de\", \"en\")\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    将Iterator类型的Dataset转为Map类型的Dataset。如果你不熟悉，可以参考：\n",
    "    https://blog.csdn.net/zhaohongfei_358/article/details/122742656\n",
    "\n",
    "    经过测试，发现其实不转也可以。效果没差别\n",
    "    \"\"\"\n",
    "    train_iter_map = to_map_style_dataset(train_iter)\n",
    "    valid_iter_map = to_map_style_dataset(valid_iter)\n",
    "\n",
    "    # 构建DataLoader，若DataLoader不熟悉，请参考文章：\n",
    "    # https://blog.csdn.net/zhaohongfei_358/article/details/122742656\n",
    "    train_dataloader = DataLoader(\n",
    "        train_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a5240",
   "metadata": {
    "id": "852a5240"
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b763750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:25.181295Z",
     "iopub.status.busy": "2022-05-02T01:25:25.180613Z",
     "iopub.status.idle": "2022-05-02T01:25:25.183519Z",
     "shell.execute_reply": "2022-05-02T01:25:25.183983Z"
    },
    "id": "1b763750"
   },
   "outputs": [],
   "source": [
    "def train_worker(\n",
    "    device,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    config,\n",
    "    is_distributed=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "    :param device: cpu或cuda\n",
    "    :param vocab_src: 源词典，本例中为德语词典\n",
    "    :param vocab_tgt: 目标词典，本例中为英语词典\n",
    "    :param spacy_de: 德语分词器\n",
    "    :param spacy_en: 英语分词器\n",
    "    :param config: 一个保存了配置参数的dict，例如学习率啥的\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Train worker process using device: {device} for training\")\n",
    "\n",
    "    # 找出目标词典中‘<blank>’所对应的index\n",
    "    pad_idx = vocab_tgt[\"<blank>\"]\n",
    "    # 设置词向量大小。\n",
    "    d_model = 512\n",
    "    # 构建模型，Layer数为6\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.to(device)\n",
    "\n",
    "    # 定义损失函数\n",
    "    criterion = LabelSmoothing(\n",
    "        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "    criterion.to(device)\n",
    "\n",
    "    # 创建train_dataloader和valid_dataloader\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        device,\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        max_padding=config[\"max_padding\"]\n",
    "    )\n",
    "\n",
    "    # 创建Adam优化器\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "\n",
    "    # 定义Warmup学习率策略\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 创建train_state，保存训练状态\n",
    "    train_state = TrainState()\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        print(f\"[Epoch {epoch} Training ====\", flush=True)\n",
    "        _, train_state = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        展示GPU使用情况，例如：\n",
    "        | ID | GPU | MEM |\n",
    "        ------------------\n",
    "        |  0 | 11% |  6% |\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            GPUtil.showUtilization()\n",
    "\n",
    "        # 每训练一个epoch保存一次模型\n",
    "        file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # 在一个epoch后，进行模型验证\n",
    "        print(f\"[Epoch {epoch} Validation ====\")\n",
    "        model.eval()\n",
    "        # 跑验证集中的数据，看看loss有多少\n",
    "        sloss = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "        # 打印验证集的Loss\n",
    "        print(\"Validation Loss:\", sloss[0].data)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 全部epoch训练完毕后，保存模型\n",
    "    file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "    torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e81a913b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:25.195835Z",
     "iopub.status.busy": "2022-05-02T01:25:25.194849Z",
     "iopub.status.idle": "2022-05-02T01:25:28.698817Z",
     "shell.execute_reply": "2022-05-02T01:25:28.699555Z"
    },
    "lines_to_next_cell": 2,
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e81a913b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1659240635115,
     "user_tz": -480,
     "elapsed": 1224173,
     "user": {
      "displayName": "Garen Snail",
      "userId": "12776734623535172445"
     }
    },
    "outputId": "9217d759-b94d-424e-cf88-879dd8339b60"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train worker process using device: cuda for training\n",
      "[Epoch 0 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   7.65 | Tokens / Sec:  2701.9 | Learning Rate: 5.4e-07\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   7.43 | Tokens / Sec:  2726.4 | Learning Rate: 1.1e-05\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   7.01 | Tokens / Sec:  2759.2 | Learning Rate: 2.2e-05\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   6.68 | Tokens / Sec:  2730.2 | Learning Rate: 3.3e-05\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   6.44 | Tokens / Sec:  2796.8 | Learning Rate: 4.4e-05\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   6.29 | Tokens / Sec:  2766.5 | Learning Rate: 5.4e-05\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   6.18 | Tokens / Sec:  2768.2 | Learning Rate: 6.5e-05\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   5.96 | Tokens / Sec:  2695.9 | Learning Rate: 7.6e-05\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   5.82 | Tokens / Sec:  2775.1 | Learning Rate: 8.7e-05\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   5.69 | Tokens / Sec:  2714.4 | Learning Rate: 9.7e-05\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   5.30 | Tokens / Sec:  2720.0 | Learning Rate: 1.1e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   5.04 | Tokens / Sec:  2768.8 | Learning Rate: 1.2e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   4.78 | Tokens / Sec:  2749.1 | Learning Rate: 1.3e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   4.74 | Tokens / Sec:  2781.2 | Learning Rate: 1.4e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   4.68 | Tokens / Sec:  2787.6 | Learning Rate: 1.5e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   4.46 | Tokens / Sec:  2758.9 | Learning Rate: 1.6e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   4.35 | Tokens / Sec:  2782.1 | Learning Rate: 1.7e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   4.15 | Tokens / Sec:  2740.2 | Learning Rate: 1.8e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   4.16 | Tokens / Sec:  2747.7 | Learning Rate: 1.9e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   4.00 | Tokens / Sec:  2777.8 | Learning Rate: 2.0e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   4.16 | Tokens / Sec:  2777.2 | Learning Rate: 2.2e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   3.98 | Tokens / Sec:  2763.0 | Learning Rate: 2.3e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   3.84 | Tokens / Sec:  2752.8 | Learning Rate: 2.4e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 50% | 24% |\n",
      "[Epoch 0 Validation ====\n",
      "Validation Loss: tensor(3.9216, device='cuda:0')\n",
      "[Epoch 1 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   3.73 | Tokens / Sec:  2931.0 | Learning Rate: 2.4e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   3.87 | Tokens / Sec:  2809.5 | Learning Rate: 2.6e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   3.93 | Tokens / Sec:  2791.2 | Learning Rate: 2.7e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   3.87 | Tokens / Sec:  2770.3 | Learning Rate: 2.8e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   3.71 | Tokens / Sec:  2783.9 | Learning Rate: 2.9e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   3.61 | Tokens / Sec:  2755.9 | Learning Rate: 3.0e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   3.64 | Tokens / Sec:  2758.2 | Learning Rate: 3.1e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   3.66 | Tokens / Sec:  2748.6 | Learning Rate: 3.2e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   3.69 | Tokens / Sec:  2798.7 | Learning Rate: 3.3e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   3.44 | Tokens / Sec:  2760.2 | Learning Rate: 3.4e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   3.35 | Tokens / Sec:  2776.2 | Learning Rate: 3.5e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   3.46 | Tokens / Sec:  2743.5 | Learning Rate: 3.6e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   3.30 | Tokens / Sec:  2745.6 | Learning Rate: 3.7e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   3.15 | Tokens / Sec:  2760.0 | Learning Rate: 3.8e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   3.10 | Tokens / Sec:  2760.3 | Learning Rate: 4.0e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   3.16 | Tokens / Sec:  2738.2 | Learning Rate: 4.1e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   3.16 | Tokens / Sec:  2746.4 | Learning Rate: 4.2e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   3.03 | Tokens / Sec:  2739.3 | Learning Rate: 4.3e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   2.99 | Tokens / Sec:  2757.4 | Learning Rate: 4.4e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   3.17 | Tokens / Sec:  2779.3 | Learning Rate: 4.5e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   3.33 | Tokens / Sec:  2762.7 | Learning Rate: 4.6e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   2.89 | Tokens / Sec:  2774.6 | Learning Rate: 4.7e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   3.04 | Tokens / Sec:  2779.2 | Learning Rate: 4.8e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 76% | 29% |\n",
      "[Epoch 1 Validation ====\n",
      "Validation Loss: tensor(2.8539, device='cuda:0')\n",
      "[Epoch 2 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   2.94 | Tokens / Sec:  3183.2 | Learning Rate: 4.9e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   2.68 | Tokens / Sec:  2750.8 | Learning Rate: 5.0e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   2.65 | Tokens / Sec:  2742.6 | Learning Rate: 5.1e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   2.75 | Tokens / Sec:  2724.4 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   2.90 | Tokens / Sec:  2766.7 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   3.17 | Tokens / Sec:  2755.2 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   2.69 | Tokens / Sec:  2742.0 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   2.79 | Tokens / Sec:  2771.6 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   2.36 | Tokens / Sec:  2783.6 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   2.54 | Tokens / Sec:  2727.6 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   2.60 | Tokens / Sec:  2752.5 | Learning Rate: 6.0e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   2.75 | Tokens / Sec:  2736.3 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   2.58 | Tokens / Sec:  2780.8 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   2.34 | Tokens / Sec:  2777.1 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   2.59 | Tokens / Sec:  2778.8 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   2.32 | Tokens / Sec:  2798.3 | Learning Rate: 6.5e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   2.03 | Tokens / Sec:  2767.7 | Learning Rate: 6.6e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   2.30 | Tokens / Sec:  2747.4 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   2.36 | Tokens / Sec:  2807.8 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   2.42 | Tokens / Sec:  2763.8 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   2.47 | Tokens / Sec:  2809.9 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   2.30 | Tokens / Sec:  2730.3 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   2.40 | Tokens / Sec:  2815.0 | Learning Rate: 7.3e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 90% | 29% |\n",
      "[Epoch 2 Validation ====\n",
      "Validation Loss: tensor(2.1302, device='cuda:0')\n",
      "[Epoch 3 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   2.10 | Tokens / Sec:  2831.7 | Learning Rate: 7.3e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   2.24 | Tokens / Sec:  2717.3 | Learning Rate: 7.4e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   1.87 | Tokens / Sec:  2791.9 | Learning Rate: 7.5e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   1.86 | Tokens / Sec:  2733.3 | Learning Rate: 7.6e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   1.93 | Tokens / Sec:  2765.8 | Learning Rate: 7.8e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   2.13 | Tokens / Sec:  2761.7 | Learning Rate: 7.9e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   2.23 | Tokens / Sec:  2778.0 | Learning Rate: 8.0e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   2.03 | Tokens / Sec:  2789.7 | Learning Rate: 8.1e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   2.14 | Tokens / Sec:  2768.2 | Learning Rate: 8.0e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.76 | Tokens / Sec:  2783.6 | Learning Rate: 8.0e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   1.93 | Tokens / Sec:  2773.4 | Learning Rate: 7.9e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   2.00 | Tokens / Sec:  2768.1 | Learning Rate: 7.9e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   1.97 | Tokens / Sec:  2763.6 | Learning Rate: 7.8e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   1.78 | Tokens / Sec:  2758.4 | Learning Rate: 7.8e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   1.96 | Tokens / Sec:  2759.9 | Learning Rate: 7.7e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   2.06 | Tokens / Sec:  2780.3 | Learning Rate: 7.7e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   1.77 | Tokens / Sec:  2776.7 | Learning Rate: 7.6e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   1.80 | Tokens / Sec:  2786.4 | Learning Rate: 7.6e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   1.64 | Tokens / Sec:  2743.4 | Learning Rate: 7.5e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   1.76 | Tokens / Sec:  2810.6 | Learning Rate: 7.5e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   1.87 | Tokens / Sec:  2720.8 | Learning Rate: 7.4e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.78 | Tokens / Sec:  2768.6 | Learning Rate: 7.4e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.90 | Tokens / Sec:  2770.9 | Learning Rate: 7.4e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 36% | 29% |\n",
      "[Epoch 3 Validation ====\n",
      "Validation Loss: tensor(1.7499, device='cuda:0')\n",
      "[Epoch 4 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   1.57 | Tokens / Sec:  3030.7 | Learning Rate: 7.3e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   1.49 | Tokens / Sec:  2742.4 | Learning Rate: 7.3e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   1.39 | Tokens / Sec:  2738.9 | Learning Rate: 7.3e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   1.55 | Tokens / Sec:  2810.5 | Learning Rate: 7.2e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   1.37 | Tokens / Sec:  2764.6 | Learning Rate: 7.2e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   1.36 | Tokens / Sec:  2764.6 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   1.78 | Tokens / Sec:  2768.2 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   1.62 | Tokens / Sec:  2777.7 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   1.55 | Tokens / Sec:  2774.8 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.35 | Tokens / Sec:  2743.7 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   1.55 | Tokens / Sec:  2820.8 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   1.70 | Tokens / Sec:  2736.0 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   1.67 | Tokens / Sec:  2769.9 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   1.36 | Tokens / Sec:  2803.4 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   1.36 | Tokens / Sec:  2804.2 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   1.41 | Tokens / Sec:  2751.7 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   1.70 | Tokens / Sec:  2702.6 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   1.56 | Tokens / Sec:  2743.1 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   1.38 | Tokens / Sec:  2779.5 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   1.37 | Tokens / Sec:  2772.4 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   1.62 | Tokens / Sec:  2729.0 | Learning Rate: 6.6e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.72 | Tokens / Sec:  2797.5 | Learning Rate: 6.6e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.53 | Tokens / Sec:  2767.4 | Learning Rate: 6.6e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 92% | 29% |\n",
      "[Epoch 4 Validation ====\n",
      "Validation Loss: tensor(1.5844, device='cuda:0')\n",
      "[Epoch 5 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   1.32 | Tokens / Sec:  3181.0 | Learning Rate: 6.6e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   1.34 | Tokens / Sec:  2745.1 | Learning Rate: 6.5e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   1.22 | Tokens / Sec:  2770.0 | Learning Rate: 6.5e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   1.31 | Tokens / Sec:  2758.7 | Learning Rate: 6.5e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   1.40 | Tokens / Sec:  2760.5 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   1.46 | Tokens / Sec:  2779.1 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   1.13 | Tokens / Sec:  2786.3 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   1.52 | Tokens / Sec:  2741.7 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   1.33 | Tokens / Sec:  2790.5 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.23 | Tokens / Sec:  2766.7 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   1.33 | Tokens / Sec:  2739.4 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   1.15 | Tokens / Sec:  2776.6 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   1.38 | Tokens / Sec:  2762.5 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   1.23 | Tokens / Sec:  2763.5 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   1.18 | Tokens / Sec:  2774.9 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   1.33 | Tokens / Sec:  2754.4 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   1.42 | Tokens / Sec:  2771.0 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   1.31 | Tokens / Sec:  2803.6 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   1.46 | Tokens / Sec:  2780.2 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   1.05 | Tokens / Sec:  2762.0 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   1.36 | Tokens / Sec:  2742.1 | Learning Rate: 6.0e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.30 | Tokens / Sec:  2748.5 | Learning Rate: 6.0e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.30 | Tokens / Sec:  2772.7 | Learning Rate: 6.0e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 83% | 29% |\n",
      "[Epoch 5 Validation ====\n",
      "Validation Loss: tensor(1.5021, device='cuda:0')\n",
      "[Epoch 6 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   1.06 | Tokens / Sec:  3130.3 | Learning Rate: 6.0e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   1.10 | Tokens / Sec:  2760.8 | Learning Rate: 6.0e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   1.16 | Tokens / Sec:  2762.1 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   1.12 | Tokens / Sec:  2736.6 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   1.12 | Tokens / Sec:  2798.5 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   0.99 | Tokens / Sec:  2770.1 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   1.10 | Tokens / Sec:  2761.1 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   1.27 | Tokens / Sec:  2765.1 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   1.09 | Tokens / Sec:  2775.4 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.21 | Tokens / Sec:  2727.3 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   1.20 | Tokens / Sec:  2814.9 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   1.06 | Tokens / Sec:  2778.2 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   1.28 | Tokens / Sec:  2741.2 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   1.21 | Tokens / Sec:  2765.1 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   0.99 | Tokens / Sec:  2750.6 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   1.10 | Tokens / Sec:  2745.9 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   1.32 | Tokens / Sec:  2807.9 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   1.34 | Tokens / Sec:  2782.5 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   1.33 | Tokens / Sec:  2778.3 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   1.19 | Tokens / Sec:  2782.2 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   1.26 | Tokens / Sec:  2733.6 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.11 | Tokens / Sec:  2763.6 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.07 | Tokens / Sec:  2768.1 | Learning Rate: 5.6e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 53% | 29% |\n",
      "[Epoch 6 Validation ====\n",
      "Validation Loss: tensor(1.4633, device='cuda:0')\n",
      "[Epoch 7 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   1.01 | Tokens / Sec:  3214.1 | Learning Rate: 5.5e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   1.09 | Tokens / Sec:  2740.9 | Learning Rate: 5.5e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   1.02 | Tokens / Sec:  2755.4 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   1.08 | Tokens / Sec:  2748.6 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   0.90 | Tokens / Sec:  2743.2 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   0.93 | Tokens / Sec:  2779.7 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   1.16 | Tokens / Sec:  2802.3 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   1.28 | Tokens / Sec:  2785.2 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   1.06 | Tokens / Sec:  2727.5 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   1.19 | Tokens / Sec:  2753.0 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   1.00 | Tokens / Sec:  2736.0 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   1.20 | Tokens / Sec:  2772.3 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   1.14 | Tokens / Sec:  2770.8 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   1.01 | Tokens / Sec:  2778.1 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   1.11 | Tokens / Sec:  2756.6 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   1.06 | Tokens / Sec:  2760.1 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   0.95 | Tokens / Sec:  2751.6 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   1.03 | Tokens / Sec:  2751.5 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   0.90 | Tokens / Sec:  2746.5 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   1.11 | Tokens / Sec:  2790.8 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   0.89 | Tokens / Sec:  2791.0 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   1.12 | Tokens / Sec:  2801.7 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   1.03 | Tokens / Sec:  2758.8 | Learning Rate: 5.2e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 57% | 29% |\n",
      "[Epoch 7 Validation ====\n",
      "Validation Loss: tensor(1.4455, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def load_trained_model():\n",
    "    \"\"\"\n",
    "    加载模型或训练模型。\n",
    "    若没有找到模型，说明没有训练过，则进行训练\n",
    "    :return: Transformer对象，即EncoderDecoder类对象\n",
    "    \"\"\"\n",
    "\n",
    "    # 定义一些模型训练参数\n",
    "    config = {\n",
    "        \"batch_size\": 32,\n",
    "        \"num_epochs\": 8, # epoch数量\n",
    "        \"accum_iter\": 10, # 每10个batch更新一次模型参数\n",
    "        \"base_lr\": 1.0,  # 基础学习率，根据这个学习率进行warmup\n",
    "        \"max_padding\": 72, # 句子的最大长度\n",
    "        \"warmup\": 3000,  # Warmup3000次，也就是从第3000次学习率开始下降\n",
    "        \"file_prefix\": \"multi30k_model_\", # 模型文件前缀名\n",
    "    }\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model_path = \"multi30k_model_final.pt\"\n",
    "\n",
    "    # 如果模型不存在，则训练一个模型\n",
    "    if not exists(model_path):\n",
    "        train_worker(device, vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n",
    "\n",
    "    # 初始化模型实例\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    # 加载模型参数\n",
    "    model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "# 加载或训练模型\n",
    "model = load_trained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f23a5f",
   "metadata": {
    "id": "34f23a5f"
   },
   "source": [
    "# 测试结果\n",
    "\n",
    "在最后我们可以使用验证集来简单的测试一下我们的模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb2571",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:28.732320Z",
     "iopub.status.busy": "2022-05-02T01:25:28.726845Z",
     "iopub.status.idle": "2022-05-02T01:25:28.742979Z",
     "shell.execute_reply": "2022-05-02T01:25:28.742276Z"
    },
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6deb2571"
   },
   "outputs": [],
   "source": [
    "# Load data and model for output checks\n",
    "def check_outputs(\n",
    "    valid_dataloader,\n",
    "    model,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    n_examples=15,\n",
    "    pad_idx=2,\n",
    "    eos_string=\"</s>\",\n",
    "):\n",
    "    results = [()] * n_examples\n",
    "    for idx in range(n_examples):\n",
    "        print(\"\\nExample %d ========\\n\" % idx)\n",
    "        b = next(iter(valid_dataloader))\n",
    "        rb = Batch(b[0], b[1], pad_idx)\n",
    "        greedy_decode(model, rb.src, rb.src_mask, 64, 0)[0]\n",
    "\n",
    "        src_tokens = [\n",
    "            vocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx\n",
    "        ]\n",
    "        tgt_tokens = [\n",
    "            vocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx\n",
    "        ]\n",
    "\n",
    "        print(\n",
    "            \"Source Text (Input)        : \"\n",
    "            + \" \".join(src_tokens).replace(\"\\n\", \"\")\n",
    "        )\n",
    "        print(\n",
    "            \"Target Text (Ground Truth) : \"\n",
    "            + \" \".join(tgt_tokens).replace(\"\\n\", \"\")\n",
    "        )\n",
    "        model_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0]\n",
    "        model_txt = (\n",
    "            \" \".join(\n",
    "                [vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]\n",
    "            ).split(eos_string, 1)[0]\n",
    "            + eos_string\n",
    "        )\n",
    "        print(\"Model Output               : \" + model_txt.replace(\"\\n\", \"\"))\n",
    "        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_model_example(n_examples=5):\n",
    "    global vocab_src, vocab_tgt, spacy_de, spacy_en\n",
    "\n",
    "    print(\"Preparing Data ...\")\n",
    "    _, valid_dataloader = create_dataloaders(\n",
    "        torch.device(\"cpu\"),\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=1,\n",
    "        is_distributed=False,\n",
    "    )\n",
    "\n",
    "    print(\"Loading Trained Model ...\")\n",
    "\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"multi30k_model_final.pt\", map_location=torch.device(\"cpu\"))\n",
    "    )\n",
    "\n",
    "    print(\"Checking Model Outputs:\")\n",
    "    example_data = check_outputs(\n",
    "        valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples\n",
    "    )\n",
    "    return model, example_data\n",
    "\n",
    "\n",
    "run_model_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "-------完结，撒花--------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "a5GJ8We9VmRG"
   },
   "id": "a5GJ8We9VmRG"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    ""
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "FQY80gNvVmRG",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1659240635688,
     "user_tz": -480,
     "elapsed": 3,
     "user": {
      "displayName": "Garen Snail",
      "userId": "12776734623535172445"
     }
    }
   },
   "id": "FQY80gNvVmRG"
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "colab": {
   "name": "AnnotatedTransformer.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "0ef04378",
    "9c90d6ee",
    "4946c508",
    "D6O3rwhyVmQ-",
    "w22nnQ1JVmQ_",
    "JUxWJpMoVmQ_",
    "74aef3a8",
    "61hvGuKuVmQ_",
    "M5IsXwP4VmRB",
    "3af07c4b",
    "aa2e3c6e",
    "18ee6655"
   ],
   "machine_shape": "hm"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}