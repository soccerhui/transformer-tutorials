{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 本文内容\n",
    "\n",
    "本文主要包括如下内容：\n",
    "\n",
    "1. Hugging Face是什么，提供了哪些内容\n",
    "2. Hugging Face模型的使用（Transformer类库）\n",
    "3. Hugging Face数据集的使用（Datasets类库）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HuggingFace简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Hugging Face Hub](https://huggingface.co/docs/hub/index)和 Github 类似，都是Hub(社区)。Hugging Face可以说的上是机器学习界的Github。Hugging Face为用户提供了以下主要功能：\n",
    "\n",
    "- [模型仓库（Model Repository）](https://huggingface.co/docs/hub/repositories-getting-started)：Git仓库可以让你管理代码版本、开源代码。而模型仓库可以让你管理模型版本、开源模型等。使用方式与Github类似。\n",
    "- [模型（Models）](https://huggingface.co/docs/hub/models)：Hugging Face为不同的机器学习任务提供了许多[预训练好的机器学习模型](https://huggingface.co/models)供大家使用，这些模型就存储在模型仓库中。\n",
    "- [数据集（Dataset）](https://huggingface.co/docs/hub/datasets)：Hugging Face上有许多公开数据集。\n",
    "\n",
    "hugging face在NLP领域最出名，其提供的模型大多都是基于Transformer的。为了易用性，Hugging Face还为用户提供了以下几个项目：\n",
    "\n",
    "  - **Transformers**([github](https://github.com/huggingface/transformers), [官方文档](https://huggingface.co/docs/transformers/index)): Transformers提供了上千个预训练好的模型可以用于不同的任务，例如文本领域、音频领域和CV领域。该项目是HuggingFace的核心，可以说学习HuggingFace就是在学习该项目如何使用。\n",
    "  - **Datasets**([github](https://github.com/huggingface/datasets), [官方文档](https://huggingface.co/docs/datasets/index)): 一个轻量级的数据集框架，主要有两个功能：①一行代码下载和预处理常用的公开数据集； ② 快速、易用的数据预处理类库。\n",
    "  - **Accelerate**([github](https://github.com/huggingface/accelerate), [官方文档](https://huggingface.co/docs/accelerate/index)): 帮助Pytorch用户很方便的实现 multi-GPU/TPU/fp16。\n",
    "  - **Space**([链接](https://huggingface.co/spaces))：Space提供了许多好玩的深度学习应用，可以尝试玩一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hugging Face模型讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transforms简介\n",
    "\n",
    "Hugging Face Transformer是Hugging Face最核心的项目，你可以用它做以下事情：\n",
    "\n",
    "- 直接使用预训练模型进行推理\n",
    "- 提供了大量预训练模型可供使用\n",
    "- 使用预训练模型进行迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transformers安装\n",
    "\n",
    "安装Transformers非常简单，直接安装即可。\n",
    "\n",
    "```shell\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 使用Transformers进行推理\n",
    "\n",
    "如果你的任务是一个比较常见的，大概率可以直接使用Transformer提供的[Pipeline](https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/pipelines)API解决，其使用方式非常简单，可以说是直接用即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hui/tools/anaconda3/envs/langchain_3_11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/hui/tools/anaconda3/envs/langchain_3_11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/hui/tools/anaconda3/envs/langchain_3_11/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': ' quel âge êtes-vous?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "print(translator(\"How old are you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "对于部分特定任务，官方并没有提供相应的模型，但你也可以到[官网搜索模型](https://huggingface.co/models)，然后显示指定即可。在加载模型时，你有可能会因为缺少一些库而报错，这个时候，只需要安装对应的库，然后重启即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: sentencepiece in /home/hui/tools/anaconda3/envs/langchain_3_11/lib/python3.11/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hui/tools/anaconda3/envs/langchain_3_11/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '我在学习深思熟虑'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_zh\", model='Helsinki-NLP/opus-mt-en-zh')\n",
    "translator(\"I'm learning deep learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "更多Pipeline请参考[官方文档](https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/pipelines)：https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 查找Hugging Face模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "本节来介绍一下如何通过Hugging Face找到你需要的模型。\n",
    "\n",
    "首先，我们需要到来到官网的[模型模块](https://huggingface.co/models)。之后我们会看到如下界面：\n",
    "\n",
    "<img src=\"./images/hf_0.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "其主要包含三部分：\n",
    "\n",
    "1. **Filter**: 用于筛选你想要的模型\n",
    "2. **模型列表**: 展示了可使用的模型。不带前缀的是官方提供的模型，例如gpt2，而带前缀的是第三方提供的模型。\n",
    "3. **搜索框**：你可以通过搜索框按名字搜索模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "当你点进去你的模型后，你会来到如下页面：\n",
    "\n",
    "<img src=\"./images/hf_1.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "该页面主要的几个部分：\n",
    "\n",
    "1. **模型介绍（Model Card）**: 我们可以通过该文档查看该模型都提供了哪些功能，模型的表现等。\n",
    "2. **模型文件（Files and versions)**: 从该模块可以下载模型文件，一般包含多种框架的（TF、Pytorch等）模型文件和配置文件等，可以用于离线加载。\n",
    "3. **测试模型(Hosted inference API)**: 可以直接通过该模块测试自己的模型。同时Hugging Face也提供了Http API可以调用，这样就不需要本地部署了。详情请参考：https://huggingface.co/docs/api-inference/index\n",
    "4. **使用该模型的应用（Spaces using ..）**：这里展示了使用该模型的应用，可以点进去玩一玩。\n",
    "5. **代码样例（Use in Transformers）**：你可以通过该模块直接查看该模型的使用方式，直接拷贝代码到项目里就可以用了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 使用Hugging Face模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Transformers项目提供了几个简单的API帮助用户使用Hugging Face模型，而这几个简单的API统称为`AutoClass`([官方文档链接](https://huggingface.co/docs/transformers/autoclass_tutorial))，包括：\n",
    "\n",
    "1. `AutoTokenizer`: 用于文本分词\n",
    "2. `AutoFeatureExtractor`: 用于特征提取\n",
    "3. `AutoProcessor`: 用于数据处理\n",
    "4. `AutoModel`: 用于加载模型\n",
    "\n",
    "它们的使用方式均为: `AutoClass.from_pretrain(\"模型名称\")`，然后就可以用了。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1005, 1049, 4083, 2784, 4083, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer(\"I'm learning deep learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "通常一个模型会包含上述4个中的部分功能，例如，对于`bert-base-uncased`模型，就包含“分词”和“模型”两项功能，我们可以通过**代码样例（Use in Transformers）** 模块查看：\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/hf_2.png\" width=\"600\">\n",
    "\n",
    "> 也不是所有的模型都可以使用`AutoModel`，具体还要看模型的代码示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "很多情况下，Hugging Face提供的模型并不能满足我们的需要，所以我们还是要自己训练模型的。此时我们可以使用Hugging Face提供的预训练模型来进行迁移学习，本节将会介绍如何使用Hugging Face进行迁移学习。\n",
    "\n",
    "使用Hugging Face模型做迁移学习的思路和普通迁移学习几乎一致：\n",
    "\n",
    "1. 首先选择一个和你的任务类似的任务的预训练模型，或者直接选择一个任务无关的基础模型。\n",
    "2. 从原有模型中拿出主干部分(backbone)\n",
    "3. 然后接上自己的下游任务，构建成新的模型\n",
    "4. 开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "这里我以`bert-base-uncased`模型作为例子，进行一次模型参数更新操作，假设我的任务是一个二分类的情感分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "首先，我们先尝试一下运行该模型，我们将该模型的**Use in Transformers**中的样例代码拷贝过来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "之后我们需要尝试使用一下该模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 4083, 2003, 1037, 2200, 3407,  103, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Learning is a very happy [MASK].\", return_tensors='pt')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1012, 4083, 2003, 1037, 2200, 3407, 2832, 1012, 1012]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs).logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> 通常HuggingFace模型的的使用都是分两步，首先分词（其他模型可能是特征提取`AutoFeatureExtractor`等），然后将第一步的结果作为模型的入参。注意第一步通常要指定`return_tensors='pt'`来让其返回tensor类型的数据。我们也可以使用Jupyter中的`tokenizer?`方式来查看其使用方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'process'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(2832)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "这里我们得到了和页面同样的数据，\n",
    "<img src=\"./images/hf_3.png\" width=\"300\">\n",
    "模型测试完毕，接下来开始正式进入迁移学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`bert-base-uncased`的任务是`Fill-Mask`，也就是填空任务，而我们的任务是情感分类，所以我们要把原本的分类器给去掉。我们先打印一下当前模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们可以从输出中看到，`bert-base-uncased`模型由两大部分构成，`bert`和最后的分类层`cls`，我们做迁移学习，肯定是要前面的`bert`层，所以我们可以这么提取其`bert`层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们来尝试一下使用`model.bert`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0568,  0.1662,  0.0943,  ..., -0.0346, -0.0636,  0.1689],\n",
      "         [-0.0402,  0.0757,  0.1923,  ..., -0.0217, -0.0459,  0.0711],\n",
      "         [-0.1038, -0.0372,  0.5063,  ..., -0.1587,  0.0475,  0.5513],\n",
      "         ...,\n",
      "         [ 0.1763, -0.0111,  0.1922,  ...,  0.1891, -0.1079, -0.2163],\n",
      "         [ 0.8013,  0.4953, -0.2258,  ...,  0.1501, -0.7685, -0.3709],\n",
      "         [ 0.0572,  0.3405,  0.6527,  ...,  0.4695, -0.0455,  0.3055]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "torch.Size([1, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model.bert(**inputs)\n",
    "print(outputs)\n",
    "print(outputs.last_hidden_state.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "可以看到，我们得到的是bert输出的隐层信息，我们可以将该隐层信息输入到一个线性层进行情感分类，然后进行损失函数计算，进而反向传播更新参数即可。有一点需要注意，上面返回的隐层Shape为(1, 9, 768)，其中1为batch_size，9是因为tokens有9个，768是每个token对应的向量的维度。我们在使用bert进行情感分类时，通常是使用第一个token（`<bos>`）的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 定义最后的二分类线性层\n",
    "cls = nn.Sequential(\n",
    "    nn.Linear(768, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "# 使用二分类常用的Binary Cross Entropy Loss\n",
    "criteria = nn.BCELoss()\n",
    "# 这里只对最后的线性层做参数更新\n",
    "optimizer = torch.optim.SGD(cls.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 取隐层的第一个token(<bos>)的输出作为cls层的输入，然后与label进行损失计算\n",
    "loss = criteria(cls(outputs.last_hidden_state[:, 0, :]), torch.FloatTensor([[1]]))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "这样，一次参数更新就完成了，尝试把他应用到真实的项目中去吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hugging Face数据集讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Datasets类库（[github](https://github.com/huggingface/datasets), [官方文档](https://huggingface.co/docs/datasets/index)）可以让你非常方便的访问和分享数据集，也可以用来对NLP、CV、语音等任务进行评价（Evaluation metrics）."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "本节将会讲解Hugging Face数据集的使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 安装Datasets类库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "直接使用pip安装即可：\n",
    "\n",
    "```shell\n",
    "pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "如果要使用语音(Audio)数据集，则需要执行如下命令：\n",
    "\n",
    "```shell\n",
    "pip install datasets[audio]\n",
    "```\n",
    "\n",
    "同理，图片(Image)数据为：\n",
    "\n",
    "```shell\n",
    "pip install datasets[vision]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 查找数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "首先，我们需要打开[Hugging Face Datasets](https://huggingface.co/datasets)页面，与Models页面类似，这里展示了Hugging Face的数据集，可以使用标签或名称进行筛选：\n",
    "\n",
    "<img src=\"./images/hf_4.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们可以点进我们感兴趣的数据集，查看详情：\n",
    "\n",
    "<img src=\"./images/hf_5.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hugging Face的数据集通常包括多个子集(subset)，并且分成了train、validation和test三份。你可以通过预览区域查看你需要的子集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "加载Hugging Face只需要用到`datasets.load_dataset`一个方法就够了。使用方法也很简单，直接填入要加载的数据集就可以了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████████████████████████████████████████████████████████████| 8551/8551 [00:00<00:00, 599917.93 examples/s]\n",
      "Generating validation split: 100%|█████████████████████████████████████████████████████████████| 1043/1043 [00:00<00:00, 213669.00 examples/s]\n",
      "Generating test split: 100%|███████████████████████████████████████████████████████████████████| 1063/1063 [00:00<00:00, 205217.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"cola\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Hugging Face的数据集都是放在github上的，所以国内估计很难下载成功。这就要用到`load_dataset`的加载本地数据集。关于如何离线下载Hugging Face数据集，可参考[该篇文章](https://blog.csdn.net/zhaohongfei_358/article/details/126222999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们这里离线下载好数据集后，将其拷贝到当前目录下，目录结构为：\n",
    "\n",
    "```\n",
    "-- glue\n",
    "    -- dummy    # glue的数据子集就放在这个目录下\n",
    "        -- ax\n",
    "        -- cola\n",
    "        ...\n",
    "    -- dataset_infos.json\n",
    "    -- glue.py\n",
    "    -- README.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们现在开始来加载本地数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\zhaohongfei1\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029922dc34334dcda94612f8fe4c1d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(path=\"./glue\", name=\"cola\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> 对于这种有子集的数据集，必须要指定你要加载的子集名称"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "可以看到，`dataset`是一个Dict类型的，那么就可以按照dict的方式访问数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', 'idx'],\n",
       "    num_rows: 8551\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       " \"One more pseudo generalization and I'm giving up.\",\n",
       " \"One more pseudo generalization or I'm giving up.\",\n",
       " 'The more we study verbs, the crazier they get.',\n",
       " 'Day by day the facts are getting murkier.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['sentence'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['label'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "到这里，数据集入门就讲完了，更多的内容就需要在你有需要的时候自己探索了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain_3_11]",
   "language": "python",
   "name": "conda-env-langchain_3_11-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
